{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3. LSTM based Language Model.ipynb","provenance":[{"file_id":"1opzCFCVHc7hzsXbOuRoF8ZenFlayg8im","timestamp":1547195062164},{"file_id":"1xen3vjgC50ALslhxIVJVAPfyNPv0nORr","timestamp":1547136286563}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"skxNOBVngUuI","colab_type":"text"},"source":["# Assignment 3: Language Modelling with LSTM networks"]},{"cell_type":"markdown","metadata":{"id":"od8wz1GsgUuO","colab_type":"text"},"source":["In this assignment, you will implement an LSTM based language model. We strongly recommend to finish first _lab 4_, which is closely related and is much simpler."]},{"cell_type":"markdown","metadata":{"id":"LG0W24LVgUuR","colab_type":"text"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"T9YkdodygUuW","colab_type":"text"},"source":["First, let's load the data as before."]},{"cell_type":"code","metadata":{"id":"dqj5ZEmHqZHg","colab_type":"code","outputId":"1da215f1-ff8a-4a97-ed82-bd95331dab0e","executionInfo":{"status":"ok","timestamp":1582994146293,"user_tz":-60,"elapsed":1216,"user":{"displayName":"Edgar Andres","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gilad5ywmbnx4ySJs1JYVLtoByZK98Q7Fr_xNG5EA=s64","userId":"04145191647758728984"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lhVrZbAAgUuZ","colab_type":"code","colab":{}},"source":["import re\n","import random\n","\n","# Let's do 2-way positive/negative classification instead of 5-way\n","easy_label_map = {0:0, 1:0, 2:None, 3:1, 4:1}\n","\n","def load_sst_data(path):\n","    data = []\n","    with open(path) as f:\n","        for i, line in enumerate(f): \n","            example = {}\n","            # Strip out the parse information and the phrase labels---we don't need those here\n","            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n","            example['text'] = text[1:]\n","            data.append(example)\n","\n","    random.seed(1)\n","    random.shuffle(data)\n","    return data\n","\n","sst_home = 'drive/My Drive/2019-2020_labs/data/trees/'\n","training_set = load_sst_data(sst_home + '/train.txt')\n","dev_set = load_sst_data(sst_home + '/dev.txt')\n","test_set = load_sst_data(sst_home + '/test.txt')\n","\n","# Note: Unlike with k-nearest neighbors, evaluation here should be fast, and we don't need to\n","# trim down the dev and test sets. "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r7vCn9ElgUui","colab_type":"text"},"source":["Next, we'll convert the data to index vectors.\n","\n","To simplify your implementation, we'll use a fixed unrolling length of 20. This means that we'll have to expand each sentence into a sequence of 21 word indices. In the conversion process, we'll mark the start of each sentence with a special word symbol `<S>`, mark the end of each sentence (if it occurs within the first 21 words) with a special word symbol `</S>`, mark extra tokens after `</S>` with a special word symbol `<PAD>`, and mark out-of-vocabulary words with `<UNK>`, for unknown. As in the previous assignment, we'll use a very small vocabulary for this assignment, so you'll see `<UNK>` often."]},{"cell_type":"code","metadata":{"id":"HNfCXqF8gUul","colab_type":"code","colab":{}},"source":["import collections\n","import numpy as np\n","\n","def sentence_to_padded_index_sequence(datasets):\n","    '''Annotates datasets with feature vectors.'''\n","    \n","    START = \"<S>\"\n","    END = \"</S>\"\n","    END_PADDING = \"<PAD>\"\n","    UNKNOWN = \"<UNK>\"\n","    SEQ_LEN = 21\n","    \n","    # Extract vocabulary\n","    def tokenize(string):\n","        return string.lower().split()\n","    \n","    word_counter = collections.Counter()\n","    for example in datasets[0]:\n","        word_counter.update(tokenize(example['text']))\n","    \n","    vocabulary = set([word for word in word_counter if word_counter[word] > 25])\n","    vocabulary = list(vocabulary)\n","    vocabulary = [START, END, END_PADDING, UNKNOWN] + vocabulary\n","        \n","    word_indices = dict(zip(vocabulary, range(len(vocabulary))))\n","    indices_to_words = {v: k for k, v in word_indices.items()}\n","        \n","    for i, dataset in enumerate(datasets):\n","        for example in dataset:\n","            example['index_sequence'] = np.zeros((SEQ_LEN), dtype=np.int32)\n","            \n","            token_sequence = [START] + tokenize(example['text']) + [END]\n","            \n","            for i in range(SEQ_LEN):\n","                if i < len(token_sequence):\n","                    if token_sequence[i] in word_indices:\n","                        index = word_indices[token_sequence[i]]\n","                    else:\n","                        index = word_indices[UNKNOWN]\n","                else:\n","                    index = word_indices[END_PADDING]\n","                example['index_sequence'][i] = index\n","    return indices_to_words, word_indices\n","    \n","indices_to_words, word_indices = sentence_to_padded_index_sequence([training_set, dev_set, test_set])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oCU1I5qSgUus","colab_type":"code","outputId":"53a4caec-068d-44aa-bb19-5e37fa4b2f86","executionInfo":{"status":"ok","timestamp":1582994160817,"user_tz":-60,"elapsed":1970,"user":{"displayName":"Edgar Andres","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gilad5ywmbnx4ySJs1JYVLtoByZK98Q7Fr_xNG5EA=s64","userId":"04145191647758728984"}},"colab":{"base_uri":"https://localhost:8080/","height":89}},"source":["print(training_set[18])\n","print(len(word_indices))"],"execution_count":21,"outputs":[{"output_type":"stream","text":["{'text': \"It could have been something special , but two things drag it down to mediocrity -- director Clare Peploe 's misunderstanding of Marivaux 's rhythms , and Mira Sorvino 's limitations as a classical actress .\", 'index_sequence': array([  0, 160, 298, 344, 385, 295, 347, 254,  73, 191, 398,   3, 160,\n","       209,  57,   3, 142, 174,   3,   3, 284], dtype=int32)}\n","603\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5ASkTrIogUu3","colab_type":"text"},"source":["## Assignments: \n","### Part 1: Implementation"]},{"cell_type":"markdown","metadata":{"id":"I__f8rvOgUu5","colab_type":"text"},"source":["Now, using the starter code and hyperparameter values provided below, implement an LSTM language model with dropout on the non-recurrent connections. Use the standard form of the LSTM reflected in the slides (without peepholes). **You should only have to edit the marked sections of code to build the base LSTM**, though implementing dropout properly may require small changes to the main training loop and to brittle_sampler().\n","\n","**Don't use any TensorFlow code that is specifically built for RNNs**. If a TF function has 'recurrent', 'sequence', 'LSTM', or 'RNN' in its name, you should built it yourself instead of using it. (Your version will likely be much simpler, by the way, since these built in methods are powerful but fairly complex and potentially confusing.)\n","\n","We won't be evaluating our model in the conventional way (perplexity on a held-out test set) for a few reasons: to save time, because we have no baseline to compare against, and because overfitting the training set is a less immediate concern with these models than it was with sentence classifiers. Instead, we'll use the value of the cost function to make sure that the model is converging as expected, and we'll use samples drawn from the model to qualitatively evaluate it.\n","\n","**Tips**: \n","- Check the code for the GRU based sentiment classifier (lab 4), specially the part where the RNN structure is defined.\n","- You'll need to use `tf.nn.embedding_lookup()`, `tf.nn.sparse_softmax_cross_entropy_with_logits()`, and `tf.split()` at least once each. All three should be easy to Google, though the last homework and the last exercise should show examples of the first two.\n","- As before, you'll want to initialize your trained parameters using something like `tf.random_normal(..., stddev=0.1)`\n","\n","**TODOS:**\n","- **TODO1**: Define the parameters of the LSTM (check the given slides in class)\n","- **TODO2**: Build the LSTM LM (follow the instructions in the code-comments)\n"]},{"cell_type":"code","metadata":{"id":"fKishKAwgUu8","colab_type":"code","outputId":"23b18a99-e2cc-40d0-fde0-89bb597f0caa","executionInfo":{"status":"ok","timestamp":1582994163986,"user_tz":-60,"elapsed":1256,"user":{"displayName":"Edgar Andres","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gilad5ywmbnx4ySJs1JYVLtoByZK98Q7Fr_xNG5EA=s64","userId":"04145191647758728984"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","tf.__version__"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.1.0'"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"8o4syEQOgUvG","colab_type":"code","colab":{}},"source":["class LanguageModel:\n","    def __init__(self, vocab_size, sequence_length):\n","        # Define the hyperparameters\n","        self.learning_rate = 0.3  # Should be about right\n","        self.training_epochs = 250  # How long to train for - chosen to fit within class time\n","        self.display_epoch_freq = 1  # How often to test and print out statistics\n","        self.dim = 32  # The dimension of the hidden state of the RNN\n","        self.embedding_dim = 16  # The dimension of the learned word embeddings\n","        self.batch_size = 256  # Somewhat arbitrary - can be tuned, but often tune for speed, not accuracy\n","        self.vocab_size = vocab_size  # Defined by the file reader above\n","        self.sequence_length = sequence_length  # Defined by the file reader above\n","        self.rate = 0.25  # Used in dropout (at training time only, not at sampling time)\n","        \n","        #### Start main editable code block ####\n","        self.trainable_variables = []\n","\n","        # logits (probabilities) and costs calculating parameters\n","        self.W_cl = tf.Variable(tf.random.normal([self.dim, self.vocab_size], stddev=0.1))\n","        self.b_cl = tf.Variable(tf.random.normal([self.vocab_size], stddev=0.1))\n","        self.trainable_variables.append(self.W_cl)\n","        self.trainable_variables.append(self.b_cl)\n","        self.l2_lambda = 0.001\n","\n","        # Embbedings parameters\n","        self.E = tf.Variable(tf.random.normal([self.vocab_size, self.embedding_dim], stddev=0.1))\n","        self.trainable_variables.append(self.E)\n","\n","        # LSTM params \n","        self.W_f = tf.Variable(tf.random.normal([self.embedding_dim + self.dim, self.dim], stddev=0.1))\n","        self.b_f = tf.Variable(tf.random.normal([self.dim], stddev=0.1))\n","        self.W_i = tf.Variable(tf.random.normal([self.embedding_dim + self.dim, self.dim], stddev=0.1))\n","        self.b_i = tf.Variable(tf.random.normal([self.dim], stddev=0.1))\n","        self.W_c = tf.Variable(tf.random.normal([self.embedding_dim + self.dim, self.dim], stddev=0.1))\n","        self.b_c = tf.Variable(tf.random.normal([self.dim], stddev=0.1))\n","        self.W_rnn = tf.Variable(tf.random.normal([self.embedding_dim + self.dim, self.dim], stddev=0.1))\n","        self.b_rnn = tf.Variable(tf.random.normal([self.dim], stddev=0.1))\n","        self.trainable_variables.append(self.W_f)\n","        self.trainable_variables.append(self.b_f)\n","        self.trainable_variables.append(self.W_i)\n","        self.trainable_variables.append(self.b_i)\n","        self.trainable_variables.append(self.W_c)\n","        self.trainable_variables.append(self.b_c)\n","        self.trainable_variables.append(self.W_rnn)\n","        self.trainable_variables.append(self.b_rnn)\n","        \n","        # initial unrolling states\n","        self.h_zero = tf.zeros([self.batch_size, self.dim])\n","        self.c_zero = tf.zeros([self.batch_size, self.dim])\n","    \n","    def model(self,x,rate,sample=False,h_zero=None,c_zero=None):   \n","        def step(x, x_next, h_prev, c_prev):\n","            emb_x = tf.nn.embedding_lookup(params=self.E,ids=x)\n","            emb_h_prev = tf.concat([emb_x, h_prev], 1) \n","            # LsTM internal machinery ...\n","            ft = tf.nn.sigmoid(tf.matmul(emb_h_prev, self.W_f)  + self.b_f)\n","            it = tf.nn.sigmoid(tf.matmul(emb_h_prev, self.W_i)  + self.b_i)\n","            Ctc = tf.nn.tanh(tf.matmul(emb_h_prev, self.W_c)  + self.b_c)\n","            Ct = ft * c_prev + it * Ctc  \n","            ot = tf.nn.sigmoid(tf.matmul(emb_h_prev, self.W_rnn)  + self.b_rnn)\n","            ht = ot * tf.nn.tanh(Ct)\n","            # apply dropout to each RNN layer ...\n","            drop = tf.nn.dropout(ht, self.rate)\n","            # Compute the logits using one last linear layer ... (here we aim to predict the following word)\n","            logits = tf.matmul(drop, self.W_cl) + self.b_cl\n","            # here we aim to learn predicting the next word\n","            # need to provide logits shape (256, vocab)\n","            costs = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = x_next, logits=logits) \n","            return logits, costs, ht, Ct\n","        #### End main editable code block ####\n","\n","        self.x_slices = tf.split(x, self.sequence_length, 1)\n","        all_logits = []\n","        all_costs = []\n","        \n","        # self h and c contains each un - rolling step\n","        if h_zero != None and c_zero != None:\n","          self.h = [h_zero]\n","          self.c = [c_zero]\n","        else:\n","          self.h = [self.h_zero]\n","          self.c = [self.c_zero]\n","   \n","        x = tf.reshape(self.x_slices[0], [-1])\n","        #TODO unroll\n","        for t in range(self.sequence_length-1):\n","          x_t = tf.reshape(self.x_slices[t], [-1]) \n","          x_next =  tf.reshape(self.x_slices[t + 1], [-1])\n","          h = self.h[len(self.h)-1] # retrieve last layer h-1\n","          c = self.c[len(self.c)-1] # retrieve last layer c-1\n","          logits, costs, h, c = step(x_t, x_next, h, c)\n","          self.h.append(h) # append last layer ht\n","          self.c.append(c) # append last layer ct\n","          all_logits.append(logits)\n","          all_costs.append(costs)\n","          if sample:\n","              return h, c, logits\n","        # we might want to return a sample ...\n","        return all_logits,all_costs        \n","            \n","           \n","    def train(self, training_data):\n","        def get_minibatch(dataset, start_index, end_index):\n","            indices = range(start_index, end_index)\n","            vectors = np.vstack([dataset[i]['index_sequence'] for i in indices])\n","            return vectors\n","             \n","        print('Training.')\n","        # Training cycle\n","        for epoch in range(self.training_epochs):\n","            random.shuffle(training_set)\n","            avg_cost = 0.\n","            total_batch = int(len(training_set) / self.batch_size)\n","            \n","            # Loop over all batches in epoch\n","            for i in range(total_batch):\n","                # Assemble a minibatch of the next B examples\n","                minibatch_vectors = np.int32(get_minibatch(training_set, self.batch_size * i, self.batch_size * (i + 1)))\n","\n","                # Run the optimizer to take a gradient step, and also fetch the value of the \n","                # cost function for logging\n","                with tf.GradientTape() as tape:\n","                  # costs contains the crossentropy achieved for each word\n","                  _, costs = self.model(minibatch_vectors,self.rate)\n","                  # so we expand the stack to sum all word costs\n","                  costs_tensor = tf.concat([tf.expand_dims(cost, 1) for cost in costs], 1)\n","                  # we average the word costs\n","                  cost_per_example = tf.reduce_sum(costs_tensor, 1)\n","                  # we average the sequence costs\n","                  total_cost = tf.reduce_mean(cost_per_example)\n","            \n","                # This performs the main SGD update equation over the sequence costs\n","                gradients = tape.gradient(total_cost, self.trainable_variables)\n","                optimizer = tf.optimizers.SGD(self.learning_rate)\n","                # back propagate errors\n","                optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n","                                                            \n","                # Compute average loss\n","                avg_cost += total_cost / (total_batch * self.batch_size)\n","                \n","            # Display some statistics about the step\n","            if (epoch+1) % self.display_epoch_freq == 0:\n","                tf.print(\"Epoch:\", (epoch+1), \"Cost:\", avg_cost, \"Sample:\", self.sample())\n","    \n","    def sample(self):\n","        # This samples a sequence of tokens from the model starting with <S>.\n","        # We only ever run the first timestep of the model, and use an effective batch size of one\n","        # but we leave the model unrolled for multiple steps, and use the full batch size to simplify \n","        # the training code. This slows things down.\n","\n","        def brittle_sampler():\n","            # The main sampling code. Can fail randomly due to rounding errors that yield probibilities\n","            # that don't sum to one.\n","            \n","            word_indices = [0] # 0 here is the \"<S>\" symbol\n","            for i in range(self.sequence_length - 1):\n","                dummy_x = np.zeros((self.batch_size, self.sequence_length),dtype=np.int32)\n","                dummy_x[0][0] = word_indices[-1]\n","                model_h = None\n","                model_c = None\n","                if i > 0:\n","                    model_h = h\n","                    model_c = c\n","\n","                # in this case h and c represents the weigthing achieved  by the last layer \n","                # so encodes the representation of the batch\n","                # and logits contains the achieved logits matrix each word (to discard those less probable words)\n","                h, c, logits = self.model(dummy_x,0.0,sample=True,h_zero=model_h,c_zero=model_c)\n","                logits = logits[0, :] # Discard all but first batch entry\n","                exp_logits = np.exp(logits - np.max(logits))\n","                distribution = exp_logits / exp_logits.sum()\n","                sampled_index = np.flatnonzero(np.random.multinomial(1, distribution))[0]\n","                word_indices.append(sampled_index)\n","            words = [indices_to_words[index] for index in word_indices]\n","            return ' '.join(words)\n","        \n","        while True:\n","            try:\n","              sample = brittle_sampler()\n","              return sample\n","            except ValueError as e:  # Retry if we experience a random failure.\n","              pass"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c7-7pfUSgUvN","colab_type":"text"},"source":["Now let's train it.\n","\n","Once you're confident your model is doing what you want, let it run for the full 250 epochs. This will take some time—likely between five and thirty minutes. If it much longer on a reasonably modern laptop—more than an hour—that suggests serious problems with your implementation. A properly implemented model with dropout should reach an average cost of less than 0.22 quickly, and then slowly improve from there. We train the model for a fairly long time because these small improvements in cost correspond to fairly large improvements in sample quality.\n","\n","Samples from a trained models should have coherent portions, but they will not resemble interpretable English sentences. Here are three examples from a model with a cost value of 0.202:\n","\n","`<S> the good <UNK> and <UNK> and <UNK> <UNK> with predictable and <UNK> , but also does one of -lrb- <UNK>`\n","\n","`<S> <UNK> has <UNK> actors seems done <UNK> would these <UNK> <UNK> to <UNK> <UNK> <UNK> 're <UNK> to mind .`\n","\n","`<S> an action story that was because the <UNK> <UNK> are when <UNK> as ``` <UNK> '' ' it is any`\n","\n","`-lrb-` and `-rrb` are the way that left and right parentheses are represented in the corpus."]},{"cell_type":"code","metadata":{"id":"zifuF0xQgUvR","colab_type":"code","outputId":"ba45b8f5-c686-4878-f512-4fb48dbe73e7","executionInfo":{"status":"ok","timestamp":1582995762679,"user_tz":-60,"elapsed":1589256,"user":{"displayName":"Edgar Andres","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gilad5ywmbnx4ySJs1JYVLtoByZK98Q7Fr_xNG5EA=s64","userId":"04145191647758728984"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["model = LanguageModel(len(word_indices), 21)\n","model.train(training_set)"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Training.\n","Epoch: 1 Cost: 0.30790782 Sample: <S> <UNK> political <UNK> direction the <UNK> moment <UNK> <UNK> <UNK> <UNK> now honest <UNK> the funny work <UNK> <UNK> <UNK>\n","Epoch: 2 Cost: 0.264318854 Sample: <S> goes director lot <UNK> to already set movie does worst <UNK> is like of <UNK> feels <UNK> a just is\n","Epoch: 3 Cost: 0.257173806 Sample: <S> <UNK> <UNK> the tv too <UNK> become the <UNK> as the <UNK> into <UNK> , sweet <UNK> movie . </S>\n","Epoch: 4 Cost: 0.252105385 Sample: <S> the there animation <UNK> as the sad just and lost as <UNK> 's movie of us , have all attempt\n","Epoch: 5 Cost: 0.247123897 Sample: <S> there 's <UNK> an characters <UNK> over a <UNK> <UNK> intriguing <UNK> <UNK> , when them 've some <UNK> here\n","Epoch: 6 Cost: 0.243273705 Sample: <S> would a <UNK> and , nothing and <UNK> of <UNK> <UNK> like fascinating the <UNK> <UNK> to opera from character\n","Epoch: 7 Cost: 0.240564153 Sample: <S> be <UNK> and to <UNK> laugh as <UNK> <UNK> ... <UNK> and <UNK> , <UNK> <UNK> <UNK> . </S> <PAD>\n","Epoch: 8 Cost: 0.237338379 Sample: <S> has plenty a <UNK> -- performances never actor , but its , but <UNK> 's <UNK> <UNK> with an has\n","Epoch: 9 Cost: 0.235179663 Sample: <S> after <UNK> in a <UNK> an movie that mr. <UNK> <UNK> <UNK> as the film , but n't <UNK> <UNK>\n","Epoch: 10 Cost: 0.233134419 Sample: <S> with a <UNK> de <UNK> <UNK> <UNK> to <UNK> <UNK> <UNK> nothing john <UNK> <UNK> characters , but it formula\n","Epoch: 11 Cost: 0.231822252 Sample: <S> never <UNK> , <UNK> , more to mystery do its <UNK> romantic to the film it may you feel actually\n","Epoch: 12 Cost: 0.230172485 Sample: <S> this world that sure to small , home <UNK> to no <UNK> 's best of <UNK> of <UNK> on the\n","Epoch: 13 Cost: 0.228333712 Sample: <S> contrived <UNK> <UNK> <UNK> himself could a many sense of the experience of <UNK> while this and <UNK> make -rrb-\n","Epoch: 14 Cost: 0.227341771 Sample: <S> the very <UNK> , as the <UNK> of one <UNK> , director <UNK> , with it 's <UNK> of <UNK>\n","Epoch: 15 Cost: 0.226417631 Sample: <S> simply <UNK> of this of classic , it 's making you of way <UNK> by solid , <UNK> , <UNK>\n","Epoch: 16 Cost: 0.225258037 Sample: <S> it your pretty video the <UNK> of slow <UNK> in <UNK> <UNK> has work that n't <UNK> <UNK> <UNK> more\n","Epoch: 17 Cost: 0.223854899 Sample: <S> good fairly <UNK> , ca is have <UNK> makes his <UNK> <UNK> characters in the <UNK> of <UNK> <UNK> ,\n","Epoch: 18 Cost: 0.223860651 Sample: <S> the many is in a film to you last to <UNK> to <UNK> so <UNK> of <UNK> on a and\n","Epoch: 19 Cost: 0.222651318 Sample: <S> <UNK> <UNK> , <UNK> that you <UNK> to <UNK> while but a <UNK> <UNK> is <UNK> the first <UNK> old\n","Epoch: 20 Cost: 0.22184 Sample: <S> he them that would still , enough one for at life <UNK> character a enjoyable film , <UNK> themselves and\n","Epoch: 21 Cost: 0.221047506 Sample: <S> there of those set out <UNK> <UNK> and worst of <UNK> 's <UNK> 's <UNK> movie . 's <UNK> if\n","Epoch: 22 Cost: 0.220371649 Sample: <S> <UNK> <UNK> <UNK> on <UNK> -- <UNK> 's <UNK> -- <UNK> , it does have be . </S> <PAD> <PAD>\n","Epoch: 23 Cost: 0.220424473 Sample: <S> the this many rare <UNK> of like its <UNK> , the rare that is just that <UNK> again may a\n","Epoch: 24 Cost: 0.219476327 Sample: <S> the <UNK> <UNK> and <UNK> a portrait movie of <UNK> , over <UNK> , <UNK> <UNK> , and <UNK> and\n","Epoch: 25 Cost: 0.219497 Sample: <S> <UNK> makes to <UNK> it <UNK> in despite this film , but <UNK> <UNK> as there 's <UNK> who probably\n","Epoch: 26 Cost: 0.218425378 Sample: <S> ... the <UNK> genre about <UNK> <UNK> is a lot that <UNK> <UNK> <UNK> with the any dumb <UNK> at\n","Epoch: 27 Cost: 0.218416721 Sample: <S> it is 've way <UNK> to <UNK> the <UNK> at a <UNK> <UNK> <UNK> to <UNK> <UNK> flick . </S>\n","Epoch: 28 Cost: 0.217495173 Sample: <S> and <UNK> is predictable after <UNK> humor , very <UNK> in an <UNK> to make see <UNK> <UNK> 's <UNK>\n","Epoch: 29 Cost: 0.217181355 Sample: <S> -lrb- <UNK> -rrb- <UNK> <UNK> <UNK> minutes -rrb- any <UNK> <UNK> in <UNK> <UNK> mystery in the <UNK> picture 's\n","Epoch: 30 Cost: 0.21679835 Sample: <S> an <UNK> are <UNK> of <UNK> , but it does just <UNK> to <UNK> , <UNK> sad . </S> <PAD>\n","Epoch: 31 Cost: 0.216308251 Sample: <S> the actors enough work -- <UNK> got a , very <UNK> , but keep the <UNK> than <UNK> gets <UNK>\n","Epoch: 32 Cost: 0.216355637 Sample: <S> <UNK> <UNK> has whether long , <UNK> or being its <UNK> remains long me , <UNK> <UNK> lack , it\n","Epoch: 33 Cost: 0.215898469 Sample: <S> the look 's some very <UNK> horror close of <UNK> <UNK> , <UNK> cast ' <UNK> characters <UNK> by <UNK>\n","Epoch: 34 Cost: 0.215832025 Sample: <S> there 's the sense of a own a <UNK> little , what <UNK> may ` the <UNK> of the plenty\n","Epoch: 35 Cost: 0.215143755 Sample: <S> <UNK> and <UNK> works out to see <UNK> any <UNK> of film really <UNK> , <UNK> <UNK> her <UNK> and\n","Epoch: 36 Cost: 0.214730382 Sample: <S> in the <UNK> <UNK> <UNK> music written as a <UNK> story with <UNK> by our <UNK> satire -- but <UNK>\n","Epoch: 37 Cost: 0.214520156 Sample: <S> it 's an sequences <UNK> , <UNK> <UNK> <UNK> , <UNK> <UNK> and <UNK> <UNK> <UNK> starts <UNK> might entertainment\n","Epoch: 38 Cost: 0.214291215 Sample: <S> but if the <UNK> of <UNK> <UNK> last <UNK> is too <UNK> the <UNK> of the <UNK> and way that\n","Epoch: 39 Cost: 0.214175627 Sample: <S> <UNK> only the <UNK> film are <UNK> that <UNK> to <UNK> <UNK> , the <UNK> <UNK> <UNK> of <UNK> 's\n","Epoch: 40 Cost: 0.213491708 Sample: <S> better <UNK> in a <UNK> <UNK> , <UNK> <UNK> or <UNK> in you <UNK> <UNK> <UNK> itself to make it\n","Epoch: 41 Cost: 0.213460878 Sample: <S> <UNK> sometimes <UNK> by its man 's <UNK> , <UNK> <UNK> <UNK> <UNK> , they it <UNK> <UNK> at a\n","Epoch: 42 Cost: 0.212958977 Sample: <S> a film , <UNK> and <UNK> , <UNK> <UNK> , who , with seeing <UNK> <UNK> , the <UNK> -rrb-\n","Epoch: 43 Cost: 0.212971717 Sample: <S> <UNK> comes to scene far and , good <UNK> <UNK> 's <UNK> , of <UNK> <UNK> <UNK> 's <UNK> --\n","Epoch: 44 Cost: 0.212848216 Sample: <S> <UNK> 's many <UNK> to production completely but <UNK> <UNK> ' the performances movie that <UNK> how <UNK> like go\n","Epoch: 45 Cost: 0.212711513 Sample: <S> <UNK> 's <UNK> gags in what is a lot of all that the big thing <UNK> their project <UNK> .\n","Epoch: 46 Cost: 0.212301642 Sample: <S> a feature more <UNK> of an film , <UNK> emotional think , and <UNK> in <UNK> of <UNK> to other\n","Epoch: 47 Cost: 0.211921647 Sample: <S> it 's <UNK> because you who she see a movie as sometimes the real <UNK> <UNK> , but it 's\n","Epoch: 48 Cost: 0.211669207 Sample: <S> the <UNK> <UNK> are for its <UNK> , <UNK> , it <UNK> ... the <UNK> <UNK> has n't like <UNK>\n","Epoch: 49 Cost: 0.211598068 Sample: <S> <UNK> <UNK> and , but <UNK> it little as <UNK> the <UNK> of them <UNK> , a <UNK> point ,\n","Epoch: 50 Cost: 0.21124503 Sample: <S> <UNK> the <UNK> : the satire are <UNK> , and best <UNK> genre on <UNK> a think , it offers\n","Epoch: 51 Cost: 0.211149201 Sample: <S> `` film can have give this <UNK> <UNK> of <UNK> version <UNK> sort of the <UNK> <UNK> <UNK> makes a\n","Epoch: 52 Cost: 0.211110756 Sample: <S> one of a <UNK> story and not imagine enough to <UNK> its <UNK> would was a <UNK> <UNK> <UNK> those\n","Epoch: 53 Cost: 0.21080789 Sample: <S> on anyone who is one of the <UNK> , but it could the tone that 's less <UNK> and while\n","Epoch: 54 Cost: 0.211144596 Sample: <S> it 's <UNK> <UNK> a moving one of anyone exercise as as an <UNK> who is is one <UNK> of\n","Epoch: 55 Cost: 0.210458085 Sample: <S> the <UNK> <UNK> <UNK> is a visual <UNK> that go to make to watch , true <UNK> and <UNK> <UNK>\n","Epoch: 56 Cost: 0.210175037 Sample: <S> it 's <UNK> <UNK> that 's nearly <UNK> , there 's the <UNK> the <UNK> <UNK> as the recent .\n","Epoch: 57 Cost: 0.210396171 Sample: <S> has just <UNK> <UNK> comedy , it 's such a better good . is short <UNK> of <UNK> for the\n","Epoch: 58 Cost: 0.209965631 Sample: <S> <UNK> <UNK> and <UNK> as the life , the film has <UNK> them to <UNK> <UNK> in the modern <UNK>\n","Epoch: 59 Cost: 0.210116699 Sample: <S> with <UNK> that : if it is not real <UNK> , but for those <UNK> around <UNK> together time makes\n","Epoch: 60 Cost: 0.209570348 Sample: <S> it 's even but it should be <UNK> of <UNK> <UNK> , but <UNK> my <UNK> did not <UNK> what\n","Epoch: 61 Cost: 0.209311813 Sample: <S> <UNK> , the movie <UNK> <UNK> <UNK> them to have some , one becomes <UNK> <UNK> or one as <UNK>\n","Epoch: 62 Cost: 0.209494084 Sample: <S> as <UNK> of <UNK> and they , not something , not that <UNK> , they are it like <UNK> 's\n","Epoch: 63 Cost: 0.20903793 Sample: <S> an moving exercise , but the <UNK> of <UNK> political the music <UNK> boy <UNK> these more some such a\n","Epoch: 64 Cost: 0.208927795 Sample: <S> de <UNK> way to <UNK> an heart , as <UNK> 's <UNK> , and <UNK> often 're more of that\n","Epoch: 65 Cost: 0.208799452 Sample: <S> a screen rather performance to `` <UNK> yet best <UNK> '' of <UNK> <UNK> of <UNK> to everyone looking to\n","Epoch: 66 Cost: 0.20913206 Sample: <S> <UNK> and often romance is <UNK> , or well things at least a <UNK> <UNK> <UNK> ... told else <UNK>\n","Epoch: 67 Cost: 0.208588406 Sample: <S> it 's <UNK> something <UNK> , the <UNK> and <UNK> real ending has both a <UNK> documentary kids or the\n","Epoch: 68 Cost: 0.208535716 Sample: <S> a <UNK> <UNK> <UNK> , <UNK> <UNK> <UNK> that <UNK> tries <UNK> energy , <UNK> <UNK> <UNK> when there does\n","Epoch: 69 Cost: 0.208256856 Sample: <S> <UNK> <UNK> up in <UNK> ' a year that as <UNK> , and an <UNK> <UNK> rich and <UNK> ,\n","Epoch: 70 Cost: 0.208022192 Sample: <S> the best <UNK> people think -lrb- <UNK> that makes <UNK> <UNK> up -- what is after any <UNK> , there\n","Epoch: 71 Cost: 0.208061099 Sample: <S> a long <UNK> of the few <UNK> <UNK> that <UNK> <UNK> sex and <UNK> itself and young <UNK> they makes\n","Epoch: 72 Cost: 0.208172232 Sample: <S> <UNK> stuff who <UNK> a <UNK> and <UNK> in the great <UNK> <UNK> to its <UNK> films of a <UNK>\n","Epoch: 73 Cost: 0.208029404 Sample: <S> <UNK> in me through a light <UNK> out of an <UNK> characters in often <UNK> or ever special <UNK> ,\n","Epoch: 74 Cost: 0.208188102 Sample: <S> an <UNK> and a right <UNK> <UNK> to <UNK> <UNK> , they can have <UNK> <UNK> you never to see\n","Epoch: 75 Cost: 0.207706898 Sample: <S> about the right <UNK> to <UNK> <UNK> minutes , its <UNK> of the <UNK> <UNK> is the character , but\n","Epoch: 76 Cost: 0.207376271 Sample: <S> it are worth - <UNK> <UNK> , <UNK> or so good a <UNK> of otherwise another is great <UNK> --\n","Epoch: 77 Cost: 0.20748803 Sample: <S> <UNK> 's <UNK> <UNK> is <UNK> and a most sad <UNK> side , and the original story is a <UNK>\n","Epoch: 78 Cost: 0.20717 Sample: <S> is <UNK> that because no cinema , <UNK> is <UNK> <UNK> an rare too <UNK> from an <UNK> ride in\n","Epoch: 79 Cost: 0.207268253 Sample: <S> the man 's first <UNK> , <UNK> direction <UNK> the same solid <UNK> , the film i may not make\n","Epoch: 80 Cost: 0.207025215 Sample: <S> you 're once you ca not feel <UNK> to <UNK> a <UNK> <UNK> way each away <UNK> for both <UNK>\n","Epoch: 81 Cost: 0.206922531 Sample: <S> work and something as <UNK> with <UNK> <UNK> <UNK> , mr. moments and making his children boy : it is\n","Epoch: 82 Cost: 0.206803024 Sample: <S> the material or <UNK> <UNK> <UNK> down <UNK> on <UNK> 's <UNK> had <UNK> of <UNK> or <UNK> <UNK> music\n","Epoch: 83 Cost: 0.206588313 Sample: <S> they may be as very <UNK> , <UNK> <UNK> in the <UNK> of her <UNK> of in one <UNK> is\n","Epoch: 84 Cost: 0.20688881 Sample: <S> the of the case of the part american that was to ever <UNK> a movie , a <UNK> thriller if\n","Epoch: 85 Cost: 0.206476808 Sample: <S> and far <UNK> up for the cinematic <UNK> with amusing -lrb- funny -rrb- now for `` <UNK> '' is no\n","Epoch: 86 Cost: 0.206453294 Sample: <S> the home acting is an moving films on <UNK> actor and love enough with the <UNK> of <UNK> and <UNK>\n","Epoch: 87 Cost: 0.206306219 Sample: <S> ... the story only is hard to do with <UNK> <UNK> on it -- to either <UNK> <UNK> a <UNK>\n","Epoch: 88 Cost: 0.206304342 Sample: <S> a more memorable <UNK> to , there 's <UNK> as the <UNK> who 's <UNK> <UNK> a <UNK> , <UNK>\n","Epoch: 89 Cost: 0.206106693 Sample: <S> we 'm know as one of you ultimately things again up -- , this is <UNK> almost <UNK> his one\n","Epoch: 90 Cost: 0.205931067 Sample: <S> it 's not strong years for such its best that leaves us <UNK> -- the <UNK> <UNK> of her <UNK>\n","Epoch: 91 Cost: 0.20593679 Sample: <S> starts like , but <UNK> <UNK> they is in history , and you 've <UNK> it to love in the\n","Epoch: 92 Cost: 0.205971822 Sample: <S> but with the project <UNK> ` rare that 's <UNK> <UNK> 's human <UNK> is not a <UNK> and particularly\n","Epoch: 93 Cost: 0.205686644 Sample: <S> that a <UNK> <UNK> is as <UNK> as <UNK> against a these <UNK> <UNK> that will see the <UNK> scenes\n","Epoch: 94 Cost: 0.205566749 Sample: <S> to <UNK> <UNK> for -- a bit of any <UNK> jokes and the <UNK> 's characters of his years <UNK>\n","Epoch: 95 Cost: 0.205587626 Sample: <S> though <UNK> for <UNK> , <UNK> <UNK> is barely over an <UNK> pretty <UNK> your a performances , against <UNK>\n","Epoch: 96 Cost: 0.205348566 Sample: <S> overall he the more movie as a very very <UNK> <UNK> that makes up be much of this man ,\n","Epoch: 97 Cost: 0.205570638 Sample: <S> there 's a movie , <UNK> , <UNK> , while rather <UNK> to <UNK> <UNK> -- funny , why <UNK>\n","Epoch: 98 Cost: 0.205423251 Sample: <S> if best comes it so <UNK> when an whole story that delivers us <UNK> too <UNK> and <UNK> to which\n","Epoch: 99 Cost: 0.205051199 Sample: <S> there 's no seen a movie with some storytelling <UNK> <UNK> the <UNK> against another <UNK> satire and tired <UNK>\n","Epoch: 100 Cost: 0.205013305 Sample: <S> a sequel in <UNK> the <UNK> and <UNK> <UNK> , and it 's like going a bad <UNK> the plot\n","Epoch: 101 Cost: 0.205039248 Sample: <S> <UNK> <UNK> kids a <UNK> of <UNK> <UNK> and <UNK> <UNK> before do love and <UNK> down <UNK> to <UNK>\n","Epoch: 102 Cost: 0.204925314 Sample: <S> or very bad ; is the beautiful movie is <UNK> that love to have be far art it the humor\n","Epoch: 103 Cost: 0.204839528 Sample: <S> once sometimes some movies go and <UNK> , in his <UNK> proves of <UNK> <UNK> in which actors , images\n","Epoch: 104 Cost: 0.204721242 Sample: <S> despite such <UNK> gives that it 's plenty of <UNK> <UNK> , despite a energy <UNK> <UNK> of the plot\n","Epoch: 105 Cost: 0.204638079 Sample: <S> and about <UNK> <UNK> just tired , just it <UNK> on a <UNK> film that makes you <UNK> for that\n","Epoch: 106 Cost: 0.204546556 Sample: <S> the film may not fairly <UNK> an <UNK> between <UNK> , and <UNK> the old <UNK> in how <UNK> with\n","Epoch: 107 Cost: 0.204333216 Sample: <S> the movie has the <UNK> ending of which the formula <UNK> , <UNK> is <UNK> in the film 's --\n","Epoch: 108 Cost: 0.204506516 Sample: <S> does n't about a <UNK> old <UNK> <UNK> for <UNK> <UNK> that fails above in a dark and life is\n","Epoch: 109 Cost: 0.20457527 Sample: <S> a cold ii <UNK> , but it does n't have the <UNK> of <UNK> and <UNK> for his <UNK> .\n","Epoch: 110 Cost: 0.204392791 Sample: <S> <UNK> , <UNK> feels like an <UNK> in <UNK> of just <UNK> and <UNK> to work , and by so\n","Epoch: 111 Cost: 0.204277277 Sample: <S> it 's feels <UNK> from little -- it the sequel is in about <UNK> the <UNK> of <UNK> . </S>\n","Epoch: 112 Cost: 0.204356626 Sample: <S> cold to <UNK> than the world <UNK> of a <UNK> <UNK> <UNK> <UNK> , we 're rarely only quite the\n","Epoch: 113 Cost: 0.204178378 Sample: <S> <UNK> like the <UNK> <UNK> to the <UNK> <UNK> , which <UNK> is a <UNK> , but <UNK> of <UNK>\n","Epoch: 114 Cost: 0.203929603 Sample: <S> this movie <UNK> away , powerful from <UNK> , <UNK> <UNK> are that a man is no <UNK> , ...\n","Epoch: 115 Cost: 0.204204604 Sample: <S> <UNK> , it does n't <UNK> , not really <UNK> certainly was <UNK> at , <UNK> that <UNK> fans ,\n","Epoch: 116 Cost: 0.203731239 Sample: <S> it 's my real <UNK> and <UNK> <UNK> -- an <UNK> new piece of <UNK> <UNK> something <UNK> to show\n","Epoch: 117 Cost: 0.203910559 Sample: <S> it 's hard written of its <UNK> classic in the men that <UNK> stories <UNK> a <UNK> <UNK> of <UNK>\n","Epoch: 118 Cost: 0.203717485 Sample: <S> though first <UNK> are <UNK> , <UNK> a movie and <UNK> from <UNK> <UNK> it just might like a <UNK>\n","Epoch: 119 Cost: 0.203836754 Sample: <S> <UNK> <UNK> <UNK> and <UNK> because the <UNK> <UNK> and <UNK> should must be the <UNK> case of the time\n","Epoch: 120 Cost: 0.203510061 Sample: <S> what <UNK> 's <UNK> <UNK> would have seen being <UNK> director ' <UNK> a <UNK> <UNK> movie when <UNK> both\n","Epoch: 121 Cost: 0.203782111 Sample: <S> a <UNK> <UNK> <UNK> <UNK> that proves <UNK> over the sense of this comedy - <UNK> <UNK> <UNK> who <UNK>\n","Epoch: 122 Cost: 0.20347105 Sample: <S> the movie 's screenplay is so them <UNK> a script that <UNK> <UNK> <UNK> <UNK> <UNK> into its the worst\n","Epoch: 123 Cost: 0.203659132 Sample: <S> not dull -- <UNK> is <UNK> with a lack of <UNK> , <UNK> <UNK> <UNK> and an action their time\n","Epoch: 124 Cost: 0.203427374 Sample: <S> just <UNK> and too been to <UNK> some <UNK> away -- when this <UNK> , <UNK> <UNK> <UNK> that it\n","Epoch: 125 Cost: 0.203294739 Sample: <S> this <UNK> drama has have this fascinating , a <UNK> <UNK> <UNK> <UNK> in this <UNK> of any <UNK> of\n","Epoch: 126 Cost: 0.20361571 Sample: <S> ends in <UNK> for <UNK> to <UNK> <UNK> to the life of the first kids who <UNK> <UNK> for the\n","Epoch: 127 Cost: 0.203583956 Sample: <S> about most <UNK> <UNK> , <UNK> enough a <UNK> <UNK> in the <UNK> of going , <UNK> but <UNK> .\n","Epoch: 128 Cost: 0.203312516 Sample: <S> as if seems <UNK> with <UNK> , <UNK> , <UNK> to work the <UNK> above watching ideas and <UNK> out\n","Epoch: 129 Cost: 0.203277349 Sample: <S> there 's a <UNK> and act , the filmmakers you <UNK> in which <UNK> -lrb- <UNK> has never seen <UNK>\n","Epoch: 130 Cost: 0.202967674 Sample: <S> what is too <UNK> , almost the things a good crime comedy that has ? should have <UNK> the <UNK>\n","Epoch: 131 Cost: 0.202766299 Sample: <S> an <UNK> <UNK> on a <UNK> <UNK> that 's never sure that i have to day the old old theater\n","Epoch: 132 Cost: 0.202882364 Sample: <S> <UNK> <UNK> <UNK> in all the violence and <UNK> of her <UNK> , <UNK> <UNK> kids <UNK> with the plot\n","Epoch: 133 Cost: 0.203041255 Sample: <S> the film is <UNK> perfect , very <UNK> , its lack that does n't <UNK> <UNK> through the first <UNK>\n","Epoch: 134 Cost: 0.202804014 Sample: <S> the <UNK> <UNK> -lrb- <UNK> of being <UNK> <UNK> -rrb- was <UNK> to not a <UNK> you 'll <UNK> <UNK>\n","Epoch: 135 Cost: 0.20308356 Sample: <S> at <UNK> ' even is a funny of <UNK> , but he will make you out like an <UNK> the\n","Epoch: 136 Cost: 0.202927873 Sample: <S> a <UNK> premise to <UNK> a deeply <UNK> whose <UNK> , <UNK> at the <UNK> <UNK> , and <UNK> ,\n","Epoch: 137 Cost: 0.202701136 Sample: <S> -lrb- a study -rrb- <UNK> more an <UNK> and earnest drama , and <UNK> <UNK> of <UNK> between <UNK> <UNK>\n","Epoch: 138 Cost: 0.202276275 Sample: <S> what makes given its <UNK> that <UNK> the script 's <UNK> and the <UNK> does n't <UNK> as hard to\n","Epoch: 139 Cost: 0.202548876 Sample: <S> <UNK> 's direction is n't enough that funny made <UNK> and it also <UNK> a <UNK> of <UNK> that you\n","Epoch: 140 Cost: 0.202553362 Sample: <S> if you know an movie about with a <UNK> comedy , but beautifully <UNK> <UNK> and yet that <UNK> ,\n","Epoch: 141 Cost: 0.202490389 Sample: <S> <UNK> 's most <UNK> <UNK> from the <UNK> to <UNK> ... , <UNK> want from a work in <UNK> <UNK>\n","Epoch: 142 Cost: 0.202384725 Sample: <S> <UNK> into the <UNK> <UNK> <UNK> of <UNK> <UNK> , <UNK> very <UNK> and under a <UNK> of . </S>\n","Epoch: 143 Cost: 0.202453062 Sample: <S> the film 's <UNK> is that <UNK> , it has <UNK> in the <UNK> that makes this <UNK> film it\n","Epoch: 144 Cost: 0.202439487 Sample: <S> what goes audiences are so <UNK> it -- at <UNK> best , also ever itself <UNK> , contrived , ,\n","Epoch: 145 Cost: 0.20251447 Sample: <S> while -lrb- <UNK> -rrb- <UNK> down give no <UNK> crime between this <UNK> <UNK> in <UNK> of <UNK> and <UNK>\n","Epoch: 146 Cost: 0.202246964 Sample: <S> a <UNK> drama <UNK> <UNK> the characters are <UNK> enough to have hour <UNK> , and one that <UNK> <UNK>\n","Epoch: 147 Cost: 0.202460065 Sample: <S> only as a <UNK> <UNK> young , much less <UNK> tale with formula the <UNK> <UNK> of <UNK> to <UNK>\n","Epoch: 148 Cost: 0.20203276 Sample: <S> with a lot at movies it 's be enough to be really <UNK> at least <UNK> has surprisingly <UNK> violence\n","Epoch: 149 Cost: 0.202097803 Sample: <S> the <UNK> is beautiful and <UNK> <UNK> , <UNK> <UNK> looks <UNK> and <UNK> but even the <UNK> of a\n","Epoch: 150 Cost: 0.2021911 Sample: <S> <UNK> <UNK> <UNK> 's <UNK> <UNK> <UNK> of the <UNK> , <UNK> <UNK> than a <UNK> , film with <UNK>\n","Epoch: 151 Cost: 0.202356666 Sample: <S> and be <UNK> ca n't <UNK> what its stories will probably be <UNK> with half in people . </S> <PAD>\n","Epoch: 152 Cost: 0.20189397 Sample: <S> he 's far at every <UNK> one mess in <UNK> <UNK> <UNK> for <UNK> the title <UNK> on her <UNK>\n","Epoch: 153 Cost: 0.201746449 Sample: <S> what we 's a <UNK> level that could probably take in <UNK> <UNK> <UNK> , and if <UNK> with a\n","Epoch: 154 Cost: 0.201953486 Sample: <S> if you <UNK> you <UNK> to quite imagine , <UNK> but the french project is an <UNK> of <UNK> in\n","Epoch: 155 Cost: 0.201647267 Sample: <S> <UNK> <UNK> <UNK> is really , some <UNK> over <UNK> up in <UNK> for all the characters that it is\n","Epoch: 156 Cost: 0.201895148 Sample: <S> the <UNK> <UNK> in being <UNK> <UNK> <UNK> about ideas in this new drama about <UNK> <UNK> exercise at his\n","Epoch: 157 Cost: 0.201743945 Sample: <S> most of which turns for <UNK> 's problem we <UNK> the reason to <UNK> place what is so <UNK> with\n","Epoch: 158 Cost: 0.20162423 Sample: <S> it 's not <UNK> so <UNK> and <UNK> , <UNK> 's film has not <UNK> young interest with <UNK> .\n","Epoch: 159 Cost: 0.201557308 Sample: <S> the <UNK> of the history of the <UNK> <UNK> - <UNK> <UNK> , that , <UNK> <UNK> <UNK> <UNK> <UNK>\n","Epoch: 160 Cost: 0.201626644 Sample: <S> this <UNK> , has the <UNK> movie is little of every and things entertaining the being few the <UNK> <UNK>\n","Epoch: 161 Cost: 0.201742977 Sample: <S> you 've end , john as the <UNK> <UNK> is a good movie he will be feel as a occasionally\n","Epoch: 162 Cost: 0.201483 Sample: <S> <UNK> <UNK> , john <UNK> , <UNK> the <UNK> <UNK> to <UNK> on <UNK> , the mood <UNK> to <UNK>\n","Epoch: 163 Cost: 0.201336056 Sample: <S> the <UNK> , the movie is as entertaining in its <UNK> years with ` <UNK> ` <UNK> ' <UNK> for\n","Epoch: 164 Cost: 0.201502129 Sample: <S> an <UNK> <UNK> , <UNK> is more than an , predictable , the <UNK> movie at imagination ; the last\n","Epoch: 165 Cost: 0.201388046 Sample: <S> see <UNK> 's best films here <UNK> he fascinating energy , but not then next the job of <UNK> every\n","Epoch: 166 Cost: 0.201416448 Sample: <S> an <UNK> of a <UNK> <UNK> `` '' of <UNK> <UNK> ; <UNK> , making me during an tragedy .\n","Epoch: 167 Cost: 0.201427102 Sample: <S> what is <UNK> and <UNK> to dull teen its <UNK> from the <UNK> <UNK> for <UNK> ideas from family character\n","Epoch: 168 Cost: 0.201281518 Sample: <S> the way where it n't <UNK> her <UNK> story that ` the film could have <UNK> the sense of <UNK>\n","Epoch: 169 Cost: 0.201209068 Sample: <S> especially compelling us to some <UNK> <UNK> of the <UNK> effort feels like it to take <UNK> political <UNK> .\n","Epoch: 170 Cost: 0.201189578 Sample: <S> <UNK> ... a <UNK> <UNK> than <UNK> <UNK> - <UNK> ' version of an <UNK> <UNK> of <UNK> movie in\n","Epoch: 171 Cost: 0.200944692 Sample: <S> in its own end , black 's <UNK> , the animation 's hollywood <UNK> to his <UNK> as the <UNK>\n","Epoch: 172 Cost: 0.200875297 Sample: <S> we 've <UNK> : not his worst <UNK> <UNK> <UNK> plays about if she are <UNK> to get up about\n","Epoch: 173 Cost: 0.200815931 Sample: <S> only is more than an actor much dialogue , who are energy , and <UNK> <UNK> , with a <UNK>\n","Epoch: 174 Cost: 0.20104441 Sample: <S> <UNK> still has its <UNK> <UNK> bit of yet familiar and when <UNK> the movie has a pleasure debut ,\n","Epoch: 175 Cost: 0.200950831 Sample: <S> a surprisingly <UNK> movies who need to <UNK> <UNK> has a <UNK> of your best a <UNK> films , and\n","Epoch: 176 Cost: 0.200806856 Sample: <S> like nearly <UNK> , <UNK> <UNK> not <UNK> as <UNK> when `` hollywood '' is does <UNK> good film ,\n","Epoch: 177 Cost: 0.200803593 Sample: <S> but you could want star the sense since the film <UNK> is a <UNK> down that it ca no <UNK>\n","Epoch: 178 Cost: 0.200886637 Sample: <S> the true case <UNK> movie seem so <UNK> away and <UNK> will even three year reason down that n't a\n","Epoch: 179 Cost: 0.200864911 Sample: <S> it 's a <UNK> premise , wild short during <UNK> , and `` <UNK> <UNK> '' is nothing long about\n","Epoch: 180 Cost: 0.201034009 Sample: <S> an <UNK> reason lost on <UNK> viewer by -lrb- more at the sort of modern idea sense of <UNK> .\n","Epoch: 181 Cost: 0.20087719 Sample: <S> the first problem the movie 's lack half <UNK> <UNK> -lrb- from <UNK> could , the <UNK> <UNK> gets never\n","Epoch: 182 Cost: 0.200933546 Sample: <S> <UNK> and <UNK> sad on -lrb- in the <UNK> story , <UNK> <UNK> ii stories -rrb- at its <UNK> ,\n","Epoch: 183 Cost: 0.200666517 Sample: <S> <UNK> <UNK> by <UNK> , <UNK> and it 's camera <UNK> into an <UNK> <UNK> cast american <UNK> -lrb- <UNK>\n","Epoch: 184 Cost: 0.200381234 Sample: <S> what <UNK> are <UNK> to make hard down than a <UNK> , <UNK> dark <UNK> a great film here .\n","Epoch: 185 Cost: 0.200536758 Sample: <S> the <UNK> with the most of the character 's movie is <UNK> not funny out your characters of making <UNK>\n","Epoch: 186 Cost: 0.200536519 Sample: <S> though the movie 's is fact and <UNK> , <UNK> and <UNK> directed the story <UNK> <UNK> <UNK> to into\n","Epoch: 187 Cost: 0.200403288 Sample: <S> a intriguing comedy that <UNK> out <UNK> there , and <UNK> <UNK> <UNK> and both and rather , despite an\n","Epoch: 188 Cost: 0.200687259 Sample: <S> the <UNK> <UNK> of me <UNK> things <UNK> the little more <UNK> to his <UNK> <UNK> on the end of\n","Epoch: 189 Cost: 0.200474575 Sample: <S> <UNK> the cinematic right <UNK> a <UNK> to <UNK> from its <UNK> and <UNK> make whether it completely <UNK> to\n","Epoch: 190 Cost: 0.200465947 Sample: <S> how <UNK> they never less <UNK> one as the theater <UNK> <UNK> job of the <UNK> -- as if <UNK>\n","Epoch: 191 Cost: 0.20067209 Sample: <S> <UNK> <UNK> satire 's the <UNK> that comes on <UNK> look at the end <UNK> for a <UNK> <UNK> and\n","Epoch: 192 Cost: 0.200368 Sample: <S> the new life movie ' probably <UNK> . were seeing but why <UNK> ca n't a powerful <UNK> <UNK> from\n","Epoch: 193 Cost: 0.200378627 Sample: <S> the <UNK> performance 's and a big - <UNK> <UNK> in <UNK> that <UNK> the film , this <UNK> works\n","Epoch: 194 Cost: 0.200412676 Sample: <S> its <UNK> and touching performance , makes the original <UNK> being <UNK> by most energy , <UNK> up ... but\n","Epoch: 195 Cost: 0.200328156 Sample: <S> i <UNK> to <UNK> the <UNK> <UNK> where the movie of a <UNK> <UNK> and <UNK> <UNK> <UNK> line ,\n","Epoch: 196 Cost: 0.200332418 Sample: <S> the film has her <UNK> to <UNK> <UNK> <UNK> cliches together and well as as <UNK> as this special <UNK>\n","Epoch: 197 Cost: 0.20002915 Sample: <S> as every character can ... were nearly <UNK> <UNK> all <UNK> , wild , and <UNK> <UNK> <UNK> come as\n","Epoch: 198 Cost: 0.200145692 Sample: <S> -lrb- <UNK> -rrb- seeing and <UNK> picture with by <UNK> <UNK> <UNK> , boy might entertaining anyone have come to\n","Epoch: 199 Cost: 0.200260341 Sample: <S> despite all : this is one of ` the movie 's <UNK> could also be <UNK> that it is <UNK>\n","Epoch: 200 Cost: 0.200080737 Sample: <S> <UNK> the <UNK> that humor just <UNK> by a <UNK> <UNK> <UNK> for the original <UNK> entirely <UNK> . </S>\n","Epoch: 201 Cost: 0.200350985 Sample: <S> not a <UNK> <UNK> and has both <UNK> sense of being powerful genre , and not from a lot of\n","Epoch: 202 Cost: 0.200132132 Sample: <S> the powerful <UNK> , <UNK> sure 's on this <UNK> new <UNK> <UNK> of the wit <UNK> an turn <UNK>\n","Epoch: 203 Cost: 0.199917033 Sample: <S> what `` <UNK> <UNK> '' <UNK> to engaging off and with <UNK> <UNK> , <UNK> <UNK> , `` <UNK> <UNK>\n","Epoch: 204 Cost: 0.200056687 Sample: <S> <UNK> 's <UNK> about <UNK> , <UNK> <UNK> on <UNK> 's the <UNK> of the worst premise to have <UNK>\n","Epoch: 205 Cost: 0.19993791 Sample: <S> both <UNK> line , <UNK> - funny <UNK> and into too much to <UNK> hollywood <UNK> in the <UNK> <UNK>\n","Epoch: 206 Cost: 0.200003117 Sample: <S> <UNK> <UNK> is one feel a movie that 's such this <UNK> feature , <UNK> <UNK> fails off on <UNK>\n","Epoch: 207 Cost: 0.200228691 Sample: <S> dull , <UNK> and <UNK> <UNK> his <UNK> <UNK> 's <UNK> to <UNK> ... and , it getting , you\n","Epoch: 208 Cost: 0.199873298 Sample: <S> <UNK> <UNK> is about <UNK> of characters with <UNK> <UNK> <UNK> <UNK> do not <UNK> a <UNK> good just bad\n","Epoch: 209 Cost: 0.199844047 Sample: <S> you comes a <UNK> <UNK> left over your <UNK> <UNK> for going will have to anyone the film a <UNK>\n","Epoch: 210 Cost: 0.199859649 Sample: <S> <UNK> and <UNK> has us a a entertainment <UNK> of a <UNK> <UNK> at every <UNK> with the same <UNK>\n","Epoch: 211 Cost: 0.200132668 Sample: <S> i can <UNK> these <UNK> <UNK> , the bad <UNK> is in <UNK> it <UNK> in <UNK> <UNK> , visually\n","Epoch: 212 Cost: 0.199845329 Sample: <S> <UNK> up the performances <UNK> about <UNK> the <UNK> 's <UNK> , who seem told to <UNK> women humor and\n","Epoch: 213 Cost: 0.199646771 Sample: <S> a <UNK> portrait that one film was really beautiful with <UNK> , <UNK> people up the <UNK> flat , but\n","Epoch: 214 Cost: 0.199780449 Sample: <S> <UNK> most a <UNK> to go off for <UNK> <UNK> , <UNK> rare is only <UNK> . </S> <PAD> <PAD>\n","Epoch: 215 Cost: 0.199654341 Sample: <S> with the film should come , i feel like making any <UNK> <UNK> had <UNK> trying like <UNK> . '\n","Epoch: 216 Cost: 0.199864373 Sample: <S> the <UNK> is few things of <UNK> , and real something so much <UNK> film by its <UNK> cast .\n","Epoch: 217 Cost: 0.199768484 Sample: <S> <UNK> what can have <UNK> the human plot here ; the man <UNK> as a <UNK> moving comedy , but\n","Epoch: 218 Cost: 0.199490681 Sample: <S> this one <UNK> from you who <UNK> <UNK> is finally <UNK> on , people are because you will <UNK> to\n","Epoch: 219 Cost: 0.199640065 Sample: <S> it 's pretty <UNK> in <UNK> live , debut for one of its own family <UNK> <UNK> , and it\n","Epoch: 220 Cost: 0.19961673 Sample: <S> had <UNK> to <UNK> under <UNK> writing and humor and <UNK> <UNK> from any on <UNK> material sequences . </S>\n","Epoch: 221 Cost: 0.19974035 Sample: <S> it is <UNK> with <UNK> version , but that were n't silly <UNK> reason from the <UNK> boy and the\n","Epoch: 222 Cost: 0.199482098 Sample: <S> the <UNK> sweet <UNK> <UNK> is the <UNK> of <UNK> <UNK> <UNK> in the <UNK> told <UNK> <UNK> these <UNK>\n","Epoch: 223 Cost: 0.199567825 Sample: <S> director <UNK> <UNK> <UNK> to find <UNK> <UNK> <UNK> for <UNK> <UNK> writing and care of a <UNK> <UNK> <UNK>\n","Epoch: 224 Cost: 0.199409828 Sample: <S> ... more <UNK> than the <UNK> and <UNK> <UNK> , do what <UNK> as even <UNK> the face of the\n","Epoch: 225 Cost: 0.199767888 Sample: <S> the <UNK> documentary are a <UNK> <UNK> <UNK> and <UNK> <UNK> with almost the same <UNK> is actually seeing a\n","Epoch: 226 Cost: 0.19934316 Sample: <S> <UNK> 's <UNK> does are is enough to <UNK> to <UNK> <UNK> , <UNK> , who manages to into <UNK>\n","Epoch: 227 Cost: 0.19977051 Sample: <S> <UNK> <UNK> -lrb- <UNK> -rrb- its <UNK> to script , it delivers the acting wit that <UNK> <UNK> you sometimes\n","Epoch: 228 Cost: 0.19947052 Sample: <S> a <UNK> <UNK> of <UNK> <UNK> and romance who and <UNK> to what <UNK> <UNK> on a romance other in\n","Epoch: 229 Cost: 0.199249327 Sample: <S> a movie about every dull family ; what a film that is <UNK> from the <UNK> feature series <UNK> to\n","Epoch: 230 Cost: 0.199402452 Sample: <S> this really little does a thriller in a romantic story , something that feels <UNK> a <UNK> -- and <UNK>\n","Epoch: 231 Cost: 0.199330747 Sample: <S> the performances was <UNK> , <UNK> <UNK> , , few the show the material does <UNK> soap year . </S>\n","Epoch: 232 Cost: 0.199297547 Sample: <S> despite some <UNK> <UNK> , -lrb- <UNK> -rrb- may give you both that it 's hard to the kids just\n","Epoch: 233 Cost: 0.199525699 Sample: <S> he last show its <UNK> and <UNK> <UNK> , <UNK> material to the flat and the <UNK> <UNK> . </S>\n","Epoch: 234 Cost: 0.199578971 Sample: <S> ultimately else of <UNK> performances , and <UNK> that <UNK> on an some sense than <UNK> the <UNK> . </S>\n","Epoch: 235 Cost: 0.199217662 Sample: <S> <UNK> <UNK> director <UNK> <UNK> -lrb- <UNK> 's -rrb- lives and funny <UNK> at its <UNK> <UNK> , but <UNK>\n","Epoch: 236 Cost: 0.199120343 Sample: <S> this director <UNK> <UNK> films out out <UNK> been not <UNK> it , but the <UNK> <UNK> <UNK> <UNK> <UNK>\n","Epoch: 237 Cost: 0.19912535 Sample: <S> <UNK> and <UNK> of its <UNK> dialogue <UNK> , <UNK> <UNK> has <UNK> than <UNK> and <UNK> <UNK> 's <UNK>\n","Epoch: 238 Cost: 0.199109435 Sample: <S> the character thing is <UNK> -- <UNK> <UNK> , which <UNK> how <UNK> '' , and this kind of movie\n","Epoch: 239 Cost: 0.199312896 Sample: <S> ... very surprisingly <UNK> , but <UNK> as <UNK> , <UNK> <UNK> <UNK> <UNK> a <UNK> and <UNK> , <UNK>\n","Epoch: 240 Cost: 0.199103609 Sample: <S> sweet filmmakers ' subject matter to enjoy kids and we <UNK> the <UNK> at these <UNK> , the <UNK> as\n","Epoch: 241 Cost: 0.19913 Sample: <S> you can it <UNK> ideas and <UNK> in being <UNK> so before its <UNK> performance for <UNK> young <UNK> .\n","Epoch: 242 Cost: 0.198930427 Sample: <S> <UNK> ` the show ' go , <UNK> just delivers a <UNK> film that <UNK> the `` <UNK> before <UNK>\n","Epoch: 243 Cost: 0.199292436 Sample: <S> its lives , big <UNK> is <UNK> is always <UNK> <UNK> , and <UNK> with <UNK> <UNK> that <UNK> <UNK>\n","Epoch: 244 Cost: 0.199026018 Sample: <S> it <UNK> <UNK> a story for the <UNK> , as a series , <UNK> <UNK> flick , it does n't\n","Epoch: 245 Cost: 0.199105665 Sample: <S> is even like it passion for <UNK> but <UNK> , like you best was a <UNK> debut tale with so\n","Epoch: 246 Cost: 0.199060291 Sample: <S> <UNK> <UNK> humor <UNK> <UNK> -lrb- the characters love men '' was an , but <UNK> <UNK> in the whole\n","Epoch: 247 Cost: 0.199103862 Sample: <S> it 's <UNK> with a rare children that ' by the last <UNK> long . from <UNK> ' with the\n","Epoch: 248 Cost: 0.19910419 Sample: <S> with just another , <UNK> <UNK> <UNK> , <UNK> more to <UNK> and his <UNK> for <UNK> and entertainment that\n","Epoch: 249 Cost: 0.19885166 Sample: <S> with <UNK> feel a lot <UNK> , but that certainly still makes his <UNK> did up anything ever : <UNK>\n","Epoch: 250 Cost: 0.199006826 Sample: <S> makes an children who to wo n't make that the <UNK> - they also <UNK> their few original over <UNK>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2kPUX6OrgUva","colab_type":"text"},"source":["Now we can draw as many samples as we like."]},{"cell_type":"code","metadata":{"id":"94XdnKR5gUvf","colab_type":"code","outputId":"124a4bda-be7b-4c26-cf45-a9fcbc11bdb8","executionInfo":{"status":"ok","timestamp":1582995763533,"user_tz":-60,"elapsed":831,"user":{"displayName":"Edgar Andres","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gilad5ywmbnx4ySJs1JYVLtoByZK98Q7Fr_xNG5EA=s64","userId":"04145191647758728984"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["model.sample() # after running, must be launched"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"<S> it 's all real charm in <UNK> a young woman <UNK> against life <UNK> that one along in the <UNK>\""]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"markdown","metadata":{"id":"5HsJ9ONQgUvk","colab_type":"text"},"source":["### Part 2: Questions"]},{"cell_type":"markdown","metadata":{"id":"pF05DtcWgUvm","colab_type":"text"},"source":["**Question 1:** Looking at the samples that your model produced towards the end of training, point out three properties of (written) English that it seems to have learned.\n","\n","\n","1. In English, adjectives modify nouns and they appear preceding the noun. This has been learned by the model as it is shown in the following examples:\n","\n","- dull familty (229)\n","- romantic story (epoch 230)\n","- sweet filmakers' (epoch 240)\n","\n","\n","2. The model has also learned to form grammatically correct English sentences that follow the Subject-Verb-Object, where there is a subject followed by a verb and an object, like the two following examples depict:\n","\n","- the movie is as entertaining in its <UNK> years (epoch 163)\n","- One film was really beautiful (epoch 213)\n","\n","\n","3. It has also learned some specificities of the English language like the fact that when we have the verb \"feel\" + \"like\" we always need the verb to be conjugated in the gerund form. This is shown in the example below.\n","\n","- i feel like making any <UNK> (215)"]},{"cell_type":"markdown","metadata":{"id":"-MeNJ9k-gUvp","colab_type":"text"},"source":["**Question 2:** If we could make the model as big as we wanted, train as long as we wanted, and adjust or remove dropout at will, could we ever get the model to reach a cost value of 0.0? In a single sentence, say why.\n","\n","* Yes, we could achieve a cost near to 0.0, that will mean overfitting to current dataset and have a clear representation of certain language, and follows the premise introduced by ElMo and BERT systems where bigger systems achieves better scores and overfit harder the dataset provided (so, generate better Language Models), in other tasks that will lack on deployment due the unknown examples won't be well treated, but in Language Modelig task that will accurate embbedings that aren't supposed to generalize just represent as accurate as possible for further tasks."]},{"cell_type":"markdown","metadata":{"id":"VXKts8jJgUvs","colab_type":"text"},"source":["**Question 3:** Give an example of a situation where the LSTM language model's ability to propagate information across many steps (when trained for long enough, at least) would cause it to reach a better cost value than a model like a simple RNN without that ability. (Answer in one sentence or so.)\n","\n"," * LSTM cells can maintain information in memory for long periods of time (They use a set of gates to control the flow of information), so for a long time training task, LSTM can be more productive (achieve long dependency comprenhension) and efficient than RNN (could omit the not relevant information). in case of RNN, for a long time training the gradient of the loss function decays exponentially with time (the vanishing gradient problem).\n","\n"," In the practical field, the clinical domain is kind of domains which should extensively use LSTM with pre-trained embbeding due the information accesible is low, and the success of each experiment require to handle every single sentence in a manner that conjugates as information as possible, my experience insight me to use BiLSTM (certain variant of the LSTM).\n","\n"," further information about RNNs and real applications: https://addi.ehu.es/bitstream/handle/10810/37091/TFG_EdgarAndresSantamaria.pdf?sequence=1&isAllowed=y "]},{"cell_type":"markdown","metadata":{"id":"75iq_DjIgUvv","colab_type":"text"},"source":["**Question 4:** Would the model be any worse if we were to just delete unknown words instead of using an `<UNK>` token? (Answer in one sentence or so.)\n","\n"," * If we remove the unknown words, that can lead to an inconsistency in sentences provided for training because those which haven't any embbeding associated will crash the program, so we need to put a sepecial tag **< UNK >** for treating those unknown words.\n"," * The optimal solution for that peoblem is actually provided by char- based models as FLAIR, those use RNN for treating from character layer into classification stage, so we can have certain representation for all words even unknown."]},{"cell_type":"markdown","metadata":{"id":"gb-PCtUagUvy","colab_type":"text"},"source":["# Team members: \n","Edgar Andrés\n","\n","Mohammed Yassin\n","\n","Xaidé Caceres\n","\n","Radostina Peteva\n","\n","# Atribution:\n","Adapted by Oier Lopez de Lacalle and Olatz Perez de Viñaspre, based on a notebook by Sam Bowman at NYU"]}]}