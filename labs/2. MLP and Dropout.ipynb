{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2. MLP and Dropout.ipynb","provenance":[{"file_id":"1mkhICIQba8Oz0Iu3FKw1EGZTIzdFHsXv","timestamp":1541614347511}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"HzzMSlqjSGBg","colab_type":"text"},"source":["# Lab2: MLPs and Dropout"]},{"cell_type":"markdown","metadata":{"id":"oJof5oljSGBk","colab_type":"text"},"source":["First, let's load the data as before."]},{"cell_type":"code","metadata":{"id":"e8fdLvE4Ygyq","colab_type":"code","outputId":"176cb6a1-b4c9-454b-dfa3-4817d2ae557b","executionInfo":{"status":"ok","timestamp":1580493114761,"user_tz":-60,"elapsed":718,"user":{"displayName":"Edgar Andres","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCm-E9SgkmdxpRGtMF2FiR7eM5W4ySSEw1mhushhg=s64","userId":"04145191647758728984"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":35,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OoRAgDVzSGBn","colab_type":"code","outputId":"89ff899e-211e-4f24-c755-5577a452bd14","executionInfo":{"status":"ok","timestamp":1580493123254,"user_tz":-60,"elapsed":783,"user":{"displayName":"Edgar Andres","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCm-E9SgkmdxpRGtMF2FiR7eM5W4ySSEw1mhushhg=s64","userId":"04145191647758728984"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["# Load the data\n","import re\n","import random\n","\n","# Let's do 2-way positive/negative classification instead of 5-way    \n","def load_sst_data(path,\n","                  easy_label_map={0:0, 1:0, 2:None, 3:1, 4:1}):\n","    data = []\n","    with open(path) as f:\n","        for i, line in enumerate(f): \n","            example = {}\n","            example['label'] = easy_label_map[int(line[1])]\n","            if example['label'] is None:\n","                continue\n","            \n","            # Strip out the parse information and the phrase labels--\n","            # ---we don't need those here\n","            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n","            example['text'] = text[1:]\n","            data.append(example)\n","    random.seed(1)\n","    random.shuffle(data)\n","    return data\n","\n","sst_home = 'drive/My Drive/2019-2020_labs/data/trees/'\n","training_set = load_sst_data(sst_home + 'train.txt')\n","dev_set = load_sst_data(sst_home + 'dev.txt')\n","test_set = load_sst_data(sst_home + 'test.txt')\n","\n","print('Training size: {}'.format(len(training_set)))\n","print('Dev size: {}'.format(len(dev_set)))\n","print('Test size: {}'.format(len(test_set)))"],"execution_count":36,"outputs":[{"output_type":"stream","text":["Training size: 6920\n","Dev size: 872\n","Test size: 1821\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RvlDOvE9SGBy","colab_type":"text"},"source":["And extract bag-of-words feature vectors. For speed, we'll only use words that appear at least 10 times in the training set, leaving us with $|V|=1254$."]},{"cell_type":"code","metadata":{"id":"ZzJYCAobSGB0","colab_type":"code","outputId":"644edea7-615e-46cf-ea9e-7508389ddca4","executionInfo":{"status":"ok","timestamp":1580493127384,"user_tz":-60,"elapsed":1513,"user":{"displayName":"Edgar Andres","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCm-E9SgkmdxpRGtMF2FiR7eM5W4ySSEw1mhushhg=s64","userId":"04145191647758728984"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import collections\n","import numpy as np\n","\n","def feature_function(datasets):\n","    '''Annotates datasets with feature vectors.'''\n","    \n","    # Extract vocabulary\n","    def tokenize(string):\n","        return string.split()\n","    \n","    word_counter = collections.Counter()\n","    for example in datasets[0]:\n","        word_counter.update(tokenize(example['text']))\n","    \n","    vocabulary = set([word for word in word_counter if word_counter[word] > 10])\n","                                \n","    feature_names = set()\n","    for i, dataset in enumerate(datasets):\n","        for example in dataset:\n","            example['features'] = collections.defaultdict(float)\n","            \n","            # Extract features (by name) for one example\n","            word_counter = collections.Counter(tokenize(example['text']))\n","            for x in word_counter.items():\n","                if x[0] in vocabulary:\n","                    example[\"features\"][\"word_count_for_\" + x[0]] = x[1]\n","            \n","            feature_names.update(example['features'].keys())\n","                            \n","    # By now, we know what all the features will be, so we can\n","    # assign indices to them.\n","    feature_indices = dict(zip(feature_names, range(len(feature_names))))\n","    indices_to_features = {v: k for k, v in feature_indices.items()}\n","    dim = len(feature_indices)\n","                \n","    # Now we create actual vectors from those indices.\n","    for dataset in datasets:\n","        for example in dataset:\n","            example['vector'] = np.zeros((dim))\n","            for feature in example['features']:\n","                example['vector'][feature_indices[feature]] = example['features'][feature]\n","    return indices_to_features, dim\n","    \n","indices_to_features, dim = feature_function([training_set, dev_set, test_set])\n","\n","print('Vocabulary size: {}'.format(dim))\n","\n"],"execution_count":37,"outputs":[{"output_type":"stream","text":["Vocabulary size: 1254\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gUlD7zgkSGB7","colab_type":"text"},"source":["And define a batch evalution function."]},{"cell_type":"code","metadata":{"id":"A533GAkvSGB9","colab_type":"code","colab":{}},"source":["def evaluate_classifier(classifier, eval_set):\n","    correct = 0\n","    hypotheses = classifier(eval_set)\n","    for i, example in enumerate(eval_set):\n","        hypothesis = hypotheses[i]\n","        if hypothesis == example['label']:\n","            correct += 1        \n","    return correct / float(len(eval_set))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H0Qn58-YSGCD","colab_type":"text"},"source":["## Assignments\n","\n","Now for the fun part! The below should be a working implementation of logistic regression in TensorFlow.\n","\n","### Part One:\n","\n","Modify it to turn it into an MLP with two ReLU hidden layers of 50 dimensions.\n","\n","Keep in mind that initializing weight matrices with zeros causes problems in deep neural networks trained by SGD. (Why?) You should use tf.random.normal instead, with stddev=0.1.\n","\n","Note: when we initialize Wx to zeros, we get some problems on training later ...\n","\n","If your model works, it should be able to overfit, reaching about 90% accuracy *on the training set* in the first 100 epochs.\n","\n","### Part Two:\n","\n","After each hidden layer, add dropout with a 80% keep rate. You're welcome to use `tf.nn.dropout`.\n","\n","Remember that dropout behaves differently at training time and at test time. This is not automatic. You can implement in various ways, but an easy way can be this:\n","\n","- Hint: Treat the dropout rate as an input to the model, just like `x`. At training time, feed it a value of `0.2`, at test time, feed it a value of `0.0`. You can explore different dropout values.\n","\n","If dropout works, your model should overfit less, but should still perform about as well (or, hopefully, better) on the dev set."]},{"cell_type":"code","metadata":{"id":"CMJeulPiSGCF","colab_type":"code","outputId":"9c36e2e1-9da6-4a17-9c8a-3d94f3d9782f","executionInfo":{"status":"ok","timestamp":1580493134668,"user_tz":-60,"elapsed":711,"user":{"displayName":"Edgar Andres","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCm-E9SgkmdxpRGtMF2FiR7eM5W4ySSEw1mhushhg=s64","userId":"04145191647758728984"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","tf.__version__"],"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.1.0'"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"code","metadata":{"id":"EyS-IkAYSGCL","colab_type":"code","colab":{}},"source":["class logistic_regression_classifier:\n","    def __init__(self, dim):\n","        # Define the hyperparameters\n","        self.learning_rate = 0.3  # Should be about right\n","        self.training_epochs = 100  # How long to train for - chosen to fit within class time\n","        self.display_epoch_freq = 1  # How often to test and print out statistics\n","        self.dim = dim  # The number of features\n","        self.Dc = 2\n","        self.batch_size = 256  # Somewhat arbitrary - can be tuned, but often tune for speed, not accuracy\n","        \n","        # TODO: Use these.\n","        self.hidden_layer_sizes = [50, 50]\n","        self.rate = 0.2 # dropout\n","\n","        # TODO: Overwrite this section\n","        ### Start of model definition ###\n","        self.trainable_variables = []\n","         # Define (most of) the model\n","        '''Variables'''\n","        #hidden layering\n","        self.W0 = tf.Variable(tf.random.normal([self.dim, self.hidden_layer_sizes[0]], stddev=0.1), dtype='float32')\n","        self.b0 = tf.Variable(tf.zeros([self.hidden_layer_sizes[0]]), dtype='float32')\n","        #hidden layering\n","        self.W1 = tf.Variable(tf.random.normal([self.hidden_layer_sizes[0],self.hidden_layer_sizes[1]], stddev=0.1), dtype='float32')\n","        self.b1 = tf.Variable(tf.zeros([self.hidden_layer_sizes[1]]), dtype='float32')\n","        #output layering\n","        self.W2 = tf.Variable(tf.random.normal([self.hidden_layer_sizes[1], self.Dc], stddev=0.1), dtype='float32')\n","        self.b2 = tf.Variable(tf.zeros([self.Dc]), dtype='float32')\n","        #define vars to train\n","        self.trainable_variables.append(self.W0)\n","        self.trainable_variables.append(self.b0)\n","        self.trainable_variables.append(self.W1)\n","        self.trainable_variables.append(self.b1)\n","        self.trainable_variables.append(self.W2)\n","        self.trainable_variables.append(self.b2)\n","        # TODO: Overwrite this section\n","\n","    def model(self,x,rate):\n","        '''Training Computation'''\n","        # TODO: Overwrite this section\n","        #Output layer activation\n","        h0 = tf.nn.relu(tf.matmul(x, self.W0) + self.b0)\n","        h0 = tf.nn.dropout(h0,rate)\n","        h1 = tf.nn.relu(tf.matmul(h0, self.W1) + self.b1)\n","        h1 = tf.nn.dropout(h1,rate)\n","        logits = tf.matmul(h1, self.W2) + self.b2\n","        logits = tf.nn.dropout(logits,rate)\n","        # TODO: Overwrite this section\n","        ### End of model definition ###\n","        return logits\n","     \n","\n","    def train(self, training_data, dev_set):\n","        def get_minibatch(dataset, start_index, end_index):\n","            indices = range(start_index, end_index)\n","            vectors = np.float32(np.vstack([dataset[i]['vector'] for i in indices]))\n","            labels = [dataset[i]['label'] for i in indices]\n","            return vectors, labels\n","      \n","        print ('Training.')\n","\n","        # Training cycle\n","        for epoch in range(self.training_epochs):\n","            random.shuffle(training_set)\n","            avg_cost = 0.\n","            total_batch = int(len(training_set) / self.batch_size)\n","            # Loop over all batches in epoch\n","            for i in range(total_batch):\n","                # Assemble a minibatch of the next B examples\n","                minibatch_vectors, minibatch_labels = get_minibatch(training_set, \n","                                                                    self.batch_size * i, \n","                                                                    self.batch_size * (i + 1))\n","                # Run the optimizer to take a gradient step, and also fetch the value of the \n","                # cost function for logging\n","                with tf.GradientTape() as tape:\n","                  logits = self.model(minibatch_vectors,self.rate)\n","                  # Define the cost function (here, the exp and sum are built in)\n","                  cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=minibatch_labels))\n","                gradients = tape.gradient(cost, self.trainable_variables)\n","                optimizer = tf.optimizers.SGD(self.learning_rate)\n","                optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n","                # Compute average loss\n","                avg_cost += cost / total_batch\n","                \n","                # Display some statistics about the step\n","            if (epoch+1) % self.display_epoch_freq == 0:\n","                tf.print (\"Epoch:\", (epoch+1), \"Cost:\", avg_cost, \\\n","                    \"Dev acc:\", evaluate_classifier(self.classify, dev_set[0:500]), \\\n","                    \"Train acc:\", evaluate_classifier(self.classify, training_set[0:500]))\n","    \n","    def classify(self, examples):\n","        # This classifies a list of examples\n","        vectors = np.float32(np.vstack([example['vector'] for example in examples]))\n","        logits = self.model(vectors,0)\n","        return np.argmax(logits, axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sU7c7S6rSGCT","colab_type":"text"},"source":["Now let's train it."]},{"cell_type":"code","metadata":{"id":"kymCD3LkSGCW","colab_type":"code","outputId":"cea8cf72-f071-4b44-8a3e-775416d3397b","executionInfo":{"status":"ok","timestamp":1580495673469,"user_tz":-60,"elapsed":40012,"user":{"displayName":"Edgar Andres","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCm-E9SgkmdxpRGtMF2FiR7eM5W4ySSEw1mhushhg=s64","userId":"04145191647758728984"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["classifier = logistic_regression_classifier(dim)\n","classifier.train(training_set, dev_set)"],"execution_count":54,"outputs":[{"output_type":"stream","text":["Training.\n","Epoch: 1 Cost: 0.692048907 Dev acc: 0.546 Train acc: 0.49\n","Epoch: 2 Cost: 0.687921166 Dev acc: 0.546 Train acc: 0.522\n","Epoch: 3 Cost: 0.683900476 Dev acc: 0.59 Train acc: 0.59\n","Epoch: 4 Cost: 0.678781331 Dev acc: 0.596 Train acc: 0.606\n","Epoch: 5 Cost: 0.672514498 Dev acc: 0.606 Train acc: 0.594\n","Epoch: 6 Cost: 0.666981 Dev acc: 0.626 Train acc: 0.632\n","Epoch: 7 Cost: 0.659786105 Dev acc: 0.62 Train acc: 0.656\n","Epoch: 8 Cost: 0.655098617 Dev acc: 0.66 Train acc: 0.646\n","Epoch: 9 Cost: 0.640381455 Dev acc: 0.638 Train acc: 0.644\n","Epoch: 10 Cost: 0.637329817 Dev acc: 0.672 Train acc: 0.67\n","Epoch: 11 Cost: 0.630503774 Dev acc: 0.676 Train acc: 0.696\n","Epoch: 12 Cost: 0.623090327 Dev acc: 0.68 Train acc: 0.728\n","Epoch: 13 Cost: 0.614253759 Dev acc: 0.7 Train acc: 0.74\n","Epoch: 14 Cost: 0.594731152 Dev acc: 0.694 Train acc: 0.72\n","Epoch: 15 Cost: 0.586290359 Dev acc: 0.706 Train acc: 0.724\n","Epoch: 16 Cost: 0.579110682 Dev acc: 0.724 Train acc: 0.748\n","Epoch: 17 Cost: 0.561552 Dev acc: 0.724 Train acc: 0.754\n","Epoch: 18 Cost: 0.556425929 Dev acc: 0.744 Train acc: 0.744\n","Epoch: 19 Cost: 0.554214597 Dev acc: 0.738 Train acc: 0.792\n","Epoch: 20 Cost: 0.53745389 Dev acc: 0.732 Train acc: 0.738\n","Epoch: 21 Cost: 0.528836131 Dev acc: 0.716 Train acc: 0.73\n","Epoch: 22 Cost: 0.518579 Dev acc: 0.7 Train acc: 0.736\n","Epoch: 23 Cost: 0.519022 Dev acc: 0.724 Train acc: 0.74\n","Epoch: 24 Cost: 0.501626194 Dev acc: 0.73 Train acc: 0.792\n","Epoch: 25 Cost: 0.481309593 Dev acc: 0.728 Train acc: 0.772\n","Epoch: 26 Cost: 0.494852 Dev acc: 0.708 Train acc: 0.808\n","Epoch: 27 Cost: 0.483546704 Dev acc: 0.734 Train acc: 0.814\n","Epoch: 28 Cost: 0.486533523 Dev acc: 0.708 Train acc: 0.826\n","Epoch: 29 Cost: 0.458173752 Dev acc: 0.748 Train acc: 0.836\n","Epoch: 30 Cost: 0.46142602 Dev acc: 0.734 Train acc: 0.82\n","Epoch: 31 Cost: 0.438311636 Dev acc: 0.762 Train acc: 0.858\n","Epoch: 32 Cost: 0.449124902 Dev acc: 0.708 Train acc: 0.8\n","Epoch: 33 Cost: 0.406080723 Dev acc: 0.768 Train acc: 0.856\n","Epoch: 34 Cost: 0.451291263 Dev acc: 0.774 Train acc: 0.868\n","Epoch: 35 Cost: 0.410969019 Dev acc: 0.676 Train acc: 0.776\n","Epoch: 36 Cost: 0.446340561 Dev acc: 0.742 Train acc: 0.866\n","Epoch: 37 Cost: 0.397948086 Dev acc: 0.748 Train acc: 0.868\n","Epoch: 38 Cost: 0.418517411 Dev acc: 0.73 Train acc: 0.858\n","Epoch: 39 Cost: 0.39085412 Dev acc: 0.736 Train acc: 0.874\n","Epoch: 40 Cost: 0.390283853 Dev acc: 0.634 Train acc: 0.706\n","Epoch: 41 Cost: 0.409250259 Dev acc: 0.748 Train acc: 0.904\n","Epoch: 42 Cost: 0.38886109 Dev acc: 0.684 Train acc: 0.794\n","Epoch: 43 Cost: 0.366365433 Dev acc: 0.764 Train acc: 0.918\n","Epoch: 44 Cost: 0.363014042 Dev acc: 0.692 Train acc: 0.796\n","Epoch: 45 Cost: 0.393581122 Dev acc: 0.744 Train acc: 0.918\n","Epoch: 46 Cost: 0.340058118 Dev acc: 0.744 Train acc: 0.892\n","Epoch: 47 Cost: 0.384788 Dev acc: 0.718 Train acc: 0.924\n","Epoch: 48 Cost: 0.35377118 Dev acc: 0.752 Train acc: 0.924\n","Epoch: 49 Cost: 0.330031157 Dev acc: 0.762 Train acc: 0.916\n","Epoch: 50 Cost: 0.303932309 Dev acc: 0.676 Train acc: 0.778\n","Epoch: 51 Cost: 0.332264304 Dev acc: 0.702 Train acc: 0.87\n","Epoch: 52 Cost: 0.349199414 Dev acc: 0.736 Train acc: 0.91\n","Epoch: 53 Cost: 0.321593612 Dev acc: 0.732 Train acc: 0.884\n","Epoch: 54 Cost: 0.32543695 Dev acc: 0.71 Train acc: 0.888\n","Epoch: 55 Cost: 0.307187319 Dev acc: 0.718 Train acc: 0.942\n","Epoch: 56 Cost: 0.303222805 Dev acc: 0.668 Train acc: 0.788\n","Epoch: 57 Cost: 0.296851516 Dev acc: 0.756 Train acc: 0.95\n","Epoch: 58 Cost: 0.286713 Dev acc: 0.662 Train acc: 0.784\n","Epoch: 59 Cost: 0.27062574 Dev acc: 0.742 Train acc: 0.952\n","Epoch: 60 Cost: 0.341387033 Dev acc: 0.728 Train acc: 0.904\n","Epoch: 61 Cost: 0.242465213 Dev acc: 0.736 Train acc: 0.936\n","Epoch: 62 Cost: 0.212980911 Dev acc: 0.764 Train acc: 0.982\n","Epoch: 63 Cost: 0.304568708 Dev acc: 0.746 Train acc: 0.944\n","Epoch: 64 Cost: 0.279183775 Dev acc: 0.744 Train acc: 0.932\n","Epoch: 65 Cost: 0.203123227 Dev acc: 0.756 Train acc: 0.964\n","Epoch: 66 Cost: 0.25801608 Dev acc: 0.744 Train acc: 0.954\n","Epoch: 67 Cost: 0.262425959 Dev acc: 0.742 Train acc: 0.97\n","Epoch: 68 Cost: 0.182938352 Dev acc: 0.75 Train acc: 0.976\n","Epoch: 69 Cost: 0.169692829 Dev acc: 0.748 Train acc: 0.968\n","Epoch: 70 Cost: 0.310831219 Dev acc: 0.73 Train acc: 0.948\n","Epoch: 71 Cost: 0.260718942 Dev acc: 0.63 Train acc: 0.712\n","Epoch: 72 Cost: 0.322989225 Dev acc: 0.732 Train acc: 0.93\n","Epoch: 73 Cost: 0.190285325 Dev acc: 0.742 Train acc: 0.982\n","Epoch: 74 Cost: 0.146249399 Dev acc: 0.748 Train acc: 0.972\n","Epoch: 75 Cost: 0.174518302 Dev acc: 0.748 Train acc: 0.972\n","Epoch: 76 Cost: 0.168264553 Dev acc: 0.748 Train acc: 0.982\n","Epoch: 77 Cost: 0.198799908 Dev acc: 0.608 Train acc: 0.684\n","Epoch: 78 Cost: 0.246734381 Dev acc: 0.754 Train acc: 0.98\n","Epoch: 79 Cost: 0.265387 Dev acc: 0.762 Train acc: 0.956\n","Epoch: 80 Cost: 0.170133 Dev acc: 0.736 Train acc: 0.988\n","Epoch: 81 Cost: 0.13059622 Dev acc: 0.732 Train acc: 0.98\n","Epoch: 82 Cost: 0.187542558 Dev acc: 0.688 Train acc: 0.89\n","Epoch: 83 Cost: 0.176255673 Dev acc: 0.754 Train acc: 0.982\n","Epoch: 84 Cost: 0.12417043 Dev acc: 0.762 Train acc: 0.974\n","Epoch: 85 Cost: 0.314378053 Dev acc: 0.718 Train acc: 0.9\n","Epoch: 86 Cost: 0.218300357 Dev acc: 0.714 Train acc: 0.954\n","Epoch: 87 Cost: 0.144765019 Dev acc: 0.736 Train acc: 0.994\n","Epoch: 88 Cost: 0.186608702 Dev acc: 0.648 Train acc: 0.782\n","Epoch: 89 Cost: 0.194101885 Dev acc: 0.752 Train acc: 0.984\n","Epoch: 90 Cost: 0.130215526 Dev acc: 0.738 Train acc: 0.974\n","Epoch: 91 Cost: 0.274135 Dev acc: 0.744 Train acc: 0.962\n","Epoch: 92 Cost: 0.159493133 Dev acc: 0.754 Train acc: 0.992\n","Epoch: 93 Cost: 0.109646067 Dev acc: 0.75 Train acc: 0.988\n","Epoch: 94 Cost: 0.142888978 Dev acc: 0.588 Train acc: 0.708\n","Epoch: 95 Cost: 0.336798698 Dev acc: 0.754 Train acc: 0.972\n","Epoch: 96 Cost: 0.173756793 Dev acc: 0.742 Train acc: 0.986\n","Epoch: 97 Cost: 0.141436711 Dev acc: 0.736 Train acc: 0.978\n","Epoch: 98 Cost: 0.135577023 Dev acc: 0.724 Train acc: 0.98\n","Epoch: 99 Cost: 0.470955104 Dev acc: 0.69 Train acc: 0.864\n","Epoch: 100 Cost: 0.286671489 Dev acc: 0.726 Train acc: 0.958\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"F_6kMcOhSGCe","colab_type":"text"},"source":["And evaluate it."]},{"cell_type":"code","metadata":{"id":"ZkTmNJpCSGCf","colab_type":"code","outputId":"c218baa2-29ee-4a2a-890c-5aa23b13a741","executionInfo":{"status":"ok","timestamp":1580495676225,"user_tz":-60,"elapsed":768,"user":{"displayName":"Edgar Andres","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCm-E9SgkmdxpRGtMF2FiR7eM5W4ySSEw1mhushhg=s64","userId":"04145191647758728984"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["evaluate_classifier(classifier.classify, dev_set)"],"execution_count":55,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7052752293577982"]},"metadata":{"tags":[]},"execution_count":55}]},{"cell_type":"markdown","metadata":{"id":"kjBUT-D2spyT","colab_type":"text"},"source":["# Atribution:\n","Adapted by Oier Lopez de Lacalle and Olatz Perez de Vi√±aspre, based on a notebook by Sam Bowman at NYU"]}]}