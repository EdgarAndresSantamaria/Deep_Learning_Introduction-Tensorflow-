{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3. RNN based classifier.ipynb","provenance":[{"file_id":"1N3d6R4lnmdNEIh3VWdAAOeP6bQc0kSbQ","timestamp":1545915164296},{"file_id":"1K4mLe1n4WyLsDu43z3dAQlYxTec9TENC","timestamp":1545914730104}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"YxcogNx1spwp","colab_type":"text"},"source":["# Lab3: Sentiment, but slower!"]},{"cell_type":"markdown","metadata":{"id":"w7huCqsRspww","colab_type":"text"},"source":["In this assignment, you'll implement an **RNN-based sentence classifier**. Plain ol' RNNs aren't very good at sentiment classification, and they're very picky about things like learning rates. However, they're the foundation for things like LSTMs, which we'll learn about next week, and which *are* quite useful."]},{"cell_type":"markdown","metadata":{"id":"tcEY8ElAspw4","colab_type":"text"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"v2LXIRspspw_","colab_type":"text"},"source":["First, let's load the data as before."]},{"cell_type":"code","metadata":{"id":"MvjTs7K1t17g","colab_type":"code","outputId":"81def9b4-f5c1-43e7-f1aa-3e8660e7d8f5","executionInfo":{"status":"ok","timestamp":1580941516910,"user_tz":-60,"elapsed":23202,"user":{"displayName":"Edgar Andres","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCm-E9SgkmdxpRGtMF2FiR7eM5W4ySSEw1mhushhg=s64","userId":"04145191647758728984"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WWRmPqvTspxB","colab_type":"code","outputId":"1b2a31dd-c734-466e-8896-b506cc7eeae4","executionInfo":{"status":"ok","timestamp":1580941519845,"user_tz":-60,"elapsed":26120,"user":{"displayName":"Edgar Andres","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCm-E9SgkmdxpRGtMF2FiR7eM5W4ySSEw1mhushhg=s64","userId":"04145191647758728984"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["import re\n","import random\n","\n","# Let's do 2-way positive/negative classification instead of 5-way\n","def load_sst_data(path,\n","                  easy_label_map={0:0, 1:0, 2:None, 3:1, 4:1}):\n","    data = []\n","    with open(path) as f:\n","        for i, line in enumerate(f): \n","            example = {}\n","            example['label'] = easy_label_map[int(line[1])]\n","            if example['label'] is None:\n","                continue\n","            \n","            # Strip out the parse information and the phrase labels--\n","            # ---we don't need those here\n","            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n","            example['text'] = text[1:]\n","            data.append(example)\n","    \n","    random.seed(1)\n","    random.shuffle(data)\n","    return data\n","   \n","sst_home = 'drive/My Drive/2019-2020_labs/data/trees/'\n","training_set = load_sst_data(sst_home + '/train.txt')\n","dev_set = load_sst_data(sst_home + '/dev.txt')\n","test_set = load_sst_data(sst_home + '/test.txt')\n","\n","print('Training size: {}'.format(len(training_set)))\n","print('Dev size: {}'.format(len(dev_set)))\n","print('Test size: {}'.format(len(test_set)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Training size: 6920\n","Dev size: 872\n","Test size: 1821\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vaoqE0D7spxL","colab_type":"text"},"source":["Next, we'll convert the data to __index vectors__.\n","\n","To simplify your implementation, we'll use a __fixed unrolling length of 20__. In the conversion process, we'll cut off excess words (towards the left/start end of the sentence), pad short sentences (to the left) with a special word symbol `<PAD>`, and mark out-of-vocabulary words with `<UNK>`, for unknown. As in the previous assignment, we'll use a very small vocabulary for this assignment, so you'll see `<UNK>` often."]},{"cell_type":"code","metadata":{"id":"L7d0AMI6spxQ","colab_type":"code","colab":{}},"source":["import collections\n","import numpy as np\n","\n","def sentence_to_padded_index_sequence(datasets):\n","    '''Annotates datasets with feature vectors.'''\n","    \n","    PADDING = \"<PAD>\"\n","    UNKNOWN = \"<UNK>\"\n","    SEQ_LEN = 20\n","    \n","    # Extract vocabulary\n","    def tokenize(string):\n","        return string.lower().split()\n","    \n","    word_counter = collections.Counter()\n","    for example in datasets[0]:\n","        word_counter.update(tokenize(example['text']))\n","    \n","    vocabulary = set([word for word in word_counter if word_counter[word] > 10])\n","    vocabulary = list(vocabulary)\n","    vocabulary = [PADDING, UNKNOWN] + vocabulary\n","        \n","    word_indices = dict(zip(vocabulary, range(len(vocabulary))))\n","    indices_to_words = {v: k for k, v in word_indices.items()}\n","        \n","    for i, dataset in enumerate(datasets):\n","        for example in dataset:\n","            example['index_sequence'] = np.zeros((SEQ_LEN), dtype=np.int32)\n","            \n","            token_sequence = tokenize(example['text'])\n","            padding = SEQ_LEN - len(token_sequence)\n","            \n","            for i in range(SEQ_LEN):\n","                if i >= padding:\n","                    if token_sequence[i - padding] in word_indices:\n","                        index = word_indices[token_sequence[i - padding]]\n","                    else:\n","                        index = word_indices[UNKNOWN]\n","                else:\n","                    index = word_indices[PADDING]\n","                example['index_sequence'][i] = index\n","    return indices_to_words, word_indices\n","    \n","indices_to_words, word_indices = sentence_to_padded_index_sequence([training_set, dev_set, test_set])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Apdx7iJospxW","colab_type":"code","outputId":"77cf985a-4989-4e66-b4fc-b7af45e37124","executionInfo":{"status":"ok","timestamp":1580941519852,"user_tz":-60,"elapsed":26107,"user":{"displayName":"Edgar Andres","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCm-E9SgkmdxpRGtMF2FiR7eM5W4ySSEw1mhushhg=s64","userId":"04145191647758728984"}},"colab":{"base_uri":"https://localhost:8080/","height":89}},"source":["print (training_set[18])\n","print (len(word_indices))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["{'label': 1, 'text': 'As the dominant Christine , Sylvie Testud is icily brilliant .', 'index_sequence': array([   0,    0,    0,    0,    0,    0,    0,    0,    0,  123,  865,\n","          1,    1, 1105,    1,    1, 1136,    1, 1217,  729], dtype=int32)}\n","1250\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0_qTijrGspxf","colab_type":"code","colab":{}},"source":["def evaluate_classifier(classifier, eval_set):\n","    correct = 0\n","    hypotheses = classifier(eval_set)\n","    for i, example in enumerate(eval_set):\n","        hypothesis = hypotheses[i]\n","        if hypothesis == example['label']:\n","            correct += 1        \n","    return correct / float(len(eval_set))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cLcqDcjvspxm","colab_type":"text"},"source":["## Assignments: Building the RNN"]},{"cell_type":"markdown","metadata":{"id":"_uC9N01Mspxq","colab_type":"text"},"source":["Replace the TODOs in the code below to make RNN work. If it's set up properly, it should reach dev set accuracy of about 0.70 within 500 epochs with the given hyperparameters.\n","\n","You will find 3 TODOs in the code.\n","\n","### TODO 1:\n","\n","- You have to define the RNN parameters (attribute *self.dim* sets dimmension of hidden state). \n","\n","- (Hint) The paremters take input's embedding (*self.embedding_dim*) and the previous hidden state (*self.dim*) and provides the current hidden state (*self.dim*).\n","\n","### TODO 2:\n","\n","- Write a (very short) Python function that defines one step of an RNN. (Hint) In each step current input and previous hidden states are involved. \n","\n","- Recall from slides: $f(h_{t-1}, p_t) = tanh(W[h_{t-1};p_t])$. Note that input $x$ at time step $t$ is *translated* to its embedding representation. \n","\n","\n","![](https://drive.google.com/uc?id=1VNI--El3renuefGD0R7AOlcxI4ycLj4V)\n","\n","\n","### TODO 3:\n","\n","- Unroll the RNN using a *for* loop, and obtain the sentence representation with the final hidden state.\n","\n","- (Hint) Note that we are vectorizing the whole minibatch. That is, in each step we are processing all the examples in the batch together in one go. Try to understand the following two code lines:\n","\n","   $\\rightarrow$ ``self.x_slices = tf.split(self.x, self.sequence_length, 1)``\n","   \n","   $\\rightarrow$ ``self.h_zero = tf.zeros([self.batch_size, self.dim])``\n","   \n","- (Hint) It might be a good idea to reshape (tf.reshape) the tensor at step t in a single tensor. "]},{"cell_type":"code","metadata":{"id":"tG8H7bYgspxw","colab_type":"code","outputId":"a4e0f586-641a-479d-b73a-77c330e65016","executionInfo":{"status":"ok","timestamp":1580941528420,"user_tz":-60,"elapsed":34657,"user":{"displayName":"Edgar Andres","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCm-E9SgkmdxpRGtMF2FiR7eM5W4ySSEw1mhushhg=s64","userId":"04145191647758728984"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","tf.__version__"],"execution_count":0,"outputs":[{"output_type":"stream","text":["TensorFlow 2.x selected.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'2.1.0'"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"piE_Cr6zspx6","colab_type":"code","colab":{}},"source":["class RNNSentimentClassifier:\n","    def __init__(self, vocab_size, sequence_length):\n","        # Define the hyperparameters\n","        self.learning_rate = 0.2  # Should be about right\n","        self.training_epochs = 500  # How long to train for - chosen to fit within class time\n","        self.display_epoch_freq = 5  # How often to test and print out statistics\n","        self.dim = 24  # The dimension of the hidden state of the RNN\n","        self.embedding_dim = 8  # The dimension of the learned word embeddings\n","        self.batch_size = 256  # Somewhat arbitrary - can be tuned, but often tune for speed, not accuracy\n","        self.vocab_size = vocab_size  # Defined by the file reader above\n","        self.sequence_length = sequence_length  # Defined by the file reader above\n","        self.l2_lambda = 0.001\n","        \n","        self.trainable_variables = []\n","\n","        # Define the parameters\n","        self.E = tf.Variable(tf.random.normal([self.vocab_size, self.embedding_dim], stddev=0.1)) # embeddings\n","        self.trainable_variables.append(self.E)\n","        \n","        self.W_cl = tf.Variable(tf.random.normal([self.dim, 2], stddev=0.1))\n","        self.b_cl = tf.Variable(tf.random.normal([2], stddev=0.1))\n","        self.trainable_variables.append(self.W_cl)\n","        self.trainable_variables.append(self.b_cl)\n","        \n","        # TODO 1: Define the RNN parameters\n","        self.W_rnn = tf.Variable(tf.random.normal([self.embedding_dim+self.dim, self.dim], stddev=0.1))\n","        self.b = tf.Variable(tf.random.normal([self.dim], stddev=0.1))\n","\n","        self.trainable_variables.append(self.W_rnn)\n","        self.trainable_variables.append(self.b)\n","        \n","    def model(self,x):\n","  \n","        # Split up the inputs into individual tensors\n","        self.x_slices = tf.split(x, self.sequence_length, 1)\n","    \n","        # Define the start state of the RNN\n","        self.h_zero = tf.zeros([self.batch_size, self.dim])  \n","        \n","        # TODO 2: Write a (very short) Python function that defines one step of an RNN\n","        def step(x, h_prev):\n","            x = tf.nn.embedding_lookup(self.E, x)\n","            return tf.math.tanh(tf.matmul(tf.concat([h_prev,x],1),self.W_rnn) + self.b)\n","        \n","        # TODO 3: Unroll the RNN using a for loop, and obtain the sentence representation with the final hidden state\n","        sentence_representation =  self.h_zero\n","        for word_slice in self.x_slices:\n","          word_slice = tf.reshape(word_slice, [-1]) \n","          sentence_representation = step(word_slice , sentence_representation)\n","\n","        # Compute the logits using one last linear layer\n","        logits = tf.matmul(sentence_representation, self.W_cl) + self.b_cl\n","        return logits\n","\n","    def train(self, training_data, dev_set):\n","        def get_minibatch(dataset, start_index, end_index):\n","            indices = range(start_index, end_index)\n","            vectors = np.vstack([dataset[i]['index_sequence'] for i in indices])\n","            labels = [dataset[i]['label'] for i in indices]\n","            return vectors, labels\n","      \n","        print('Training.')\n","\n","        # Training cycle\n","        for epoch in range(self.training_epochs):\n","            random.shuffle(training_set)\n","            avg_cost = 0.\n","            total_batch = int(len(training_set) / self.batch_size)\n","            \n","            # Loop over all batches in epoch\n","            for i in range(total_batch):\n","                # Assemble a minibatch of the next B examples\n","                minibatch_vectors, minibatch_labels = get_minibatch(training_set, \n","                                                                    self.batch_size * i, \n","                                                                    self.batch_size * (i + 1))\n","\n","                # Run the optimizer to take a gradient step, and also fetch the value of the \n","                # cost function for logging\n","                with tf.GradientTape() as tape:\n","                  logits = self.model(minibatch_vectors)\n","                \n","                  # Define the L2 cost\n","                  self.l2_cost = self.l2_lambda * (tf.reduce_sum(tf.square(self.W_rnn)) +\n","                                                  tf.reduce_sum(tf.square(self.W_cl)))\n","\n","                  # Define the cost function (here, the softmax exp and sum are built in)\n","                  total_cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=minibatch_labels, logits=logits) + self.l2_cost)\n","        \n","                # This  performs the main SGD update equation with gradient clipping\n","                optimizer = tf.optimizers.SGD(self.learning_rate)\n","                gradients = tape.gradient(total_cost, self.trainable_variables)\n","                gvs = zip(gradients, self.trainable_variables)\n","                capped_gvs = [(tf.clip_by_norm(grad, 5.0), var) for grad, var in gvs if grad is not None]\n","                optimizer.apply_gradients(capped_gvs)\n","                                                                            \n","                # Compute average loss\n","                avg_cost += total_cost / total_batch\n","                \n","            # Display some statistics about the step\n","            # Evaluating only one batch worth of data -- simplifies implementation slightly\n","            if (epoch+1) % self.display_epoch_freq == 0:\n","                tf.print(\"Epoch:\", (epoch+1), \"Cost:\", avg_cost, \\\n","                    \"Dev acc:\", evaluate_classifier(self.classify, dev_set[0:256]), \\\n","                    \"Train acc:\", evaluate_classifier(self.classify, training_set[0:256]))  \n","    \n","    def classify(self, examples):\n","        # This classifies a list of examples\n","        vectors = np.vstack([example['index_sequence'] for example in examples])\n","        logits = self.model(vectors)\n","        return np.argmax(logits, axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"26SBs4iespyE","colab_type":"code","outputId":"ce2c9f36-98fd-407f-c6d9-55b4aaee461d","executionInfo":{"status":"ok","timestamp":1580942128176,"user_tz":-60,"elapsed":634395,"user":{"displayName":"Edgar Andres","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCm-E9SgkmdxpRGtMF2FiR7eM5W4ySSEw1mhushhg=s64","userId":"04145191647758728984"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["classifier = RNNSentimentClassifier(len(word_indices), 20)\n","classifier.train(training_set, dev_set)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Training.\n","Epoch: 5 Cost: 0.699618876 Dev acc: 0.55859375 Train acc: 0.546875\n","Epoch: 10 Cost: 0.698827386 Dev acc: 0.56640625 Train acc: 0.5625\n","Epoch: 15 Cost: 0.698023438 Dev acc: 0.55859375 Train acc: 0.484375\n","Epoch: 20 Cost: 0.697295129 Dev acc: 0.5546875 Train acc: 0.546875\n","Epoch: 25 Cost: 0.696720481 Dev acc: 0.5546875 Train acc: 0.52734375\n","Epoch: 30 Cost: 0.696143508 Dev acc: 0.546875 Train acc: 0.58203125\n","Epoch: 35 Cost: 0.695680499 Dev acc: 0.5546875 Train acc: 0.59375\n","Epoch: 40 Cost: 0.695176601 Dev acc: 0.5546875 Train acc: 0.55078125\n","Epoch: 45 Cost: 0.694563866 Dev acc: 0.55078125 Train acc: 0.5078125\n","Epoch: 50 Cost: 0.69394815 Dev acc: 0.546875 Train acc: 0.5390625\n","Epoch: 55 Cost: 0.693410218 Dev acc: 0.5546875 Train acc: 0.50390625\n","Epoch: 60 Cost: 0.693211 Dev acc: 0.546875 Train acc: 0.5\n","Epoch: 65 Cost: 0.692549348 Dev acc: 0.546875 Train acc: 0.51953125\n","Epoch: 70 Cost: 0.691948175 Dev acc: 0.53125 Train acc: 0.51953125\n","Epoch: 75 Cost: 0.691401 Dev acc: 0.51953125 Train acc: 0.51953125\n","Epoch: 80 Cost: 0.691031277 Dev acc: 0.55078125 Train acc: 0.515625\n","Epoch: 85 Cost: 0.690297961 Dev acc: 0.55078125 Train acc: 0.59375\n","Epoch: 90 Cost: 0.689582229 Dev acc: 0.5546875 Train acc: 0.55078125\n","Epoch: 95 Cost: 0.688389719 Dev acc: 0.5546875 Train acc: 0.546875\n","Epoch: 100 Cost: 0.687203407 Dev acc: 0.578125 Train acc: 0.5625\n","Epoch: 105 Cost: 0.684488297 Dev acc: 0.5703125 Train acc: 0.53125\n","Epoch: 110 Cost: 0.682008564 Dev acc: 0.59375 Train acc: 0.5703125\n","Epoch: 115 Cost: 0.677520931 Dev acc: 0.58984375 Train acc: 0.58984375\n","Epoch: 120 Cost: 0.67153728 Dev acc: 0.61328125 Train acc: 0.625\n","Epoch: 125 Cost: 0.664386094 Dev acc: 0.59765625 Train acc: 0.62109375\n","Epoch: 130 Cost: 0.656221867 Dev acc: 0.6015625 Train acc: 0.546875\n","Epoch: 135 Cost: 0.648470461 Dev acc: 0.59375 Train acc: 0.625\n","Epoch: 140 Cost: 0.646161675 Dev acc: 0.58203125 Train acc: 0.69921875\n","Epoch: 145 Cost: 0.637907505 Dev acc: 0.58984375 Train acc: 0.62890625\n","Epoch: 150 Cost: 0.629127502 Dev acc: 0.5625 Train acc: 0.62890625\n","Epoch: 155 Cost: 0.626100242 Dev acc: 0.57421875 Train acc: 0.609375\n","Epoch: 160 Cost: 0.625873864 Dev acc: 0.6015625 Train acc: 0.66796875\n","Epoch: 165 Cost: 0.611466646 Dev acc: 0.55078125 Train acc: 0.63671875\n","Epoch: 170 Cost: 0.610174656 Dev acc: 0.58984375 Train acc: 0.65234375\n","Epoch: 175 Cost: 0.600611567 Dev acc: 0.5625 Train acc: 0.6953125\n","Epoch: 180 Cost: 0.602438211 Dev acc: 0.60546875 Train acc: 0.609375\n","Epoch: 185 Cost: 0.591602325 Dev acc: 0.58203125 Train acc: 0.65625\n","Epoch: 190 Cost: 0.588573039 Dev acc: 0.59765625 Train acc: 0.65625\n","Epoch: 195 Cost: 0.582939744 Dev acc: 0.5703125 Train acc: 0.66796875\n","Epoch: 200 Cost: 0.574579835 Dev acc: 0.59765625 Train acc: 0.68359375\n","Epoch: 205 Cost: 0.565215468 Dev acc: 0.5859375 Train acc: 0.69921875\n","Epoch: 210 Cost: 0.561999261 Dev acc: 0.5703125 Train acc: 0.69140625\n","Epoch: 215 Cost: 0.554370701 Dev acc: 0.57421875 Train acc: 0.7578125\n","Epoch: 220 Cost: 0.555658042 Dev acc: 0.5859375 Train acc: 0.69140625\n","Epoch: 225 Cost: 0.559895933 Dev acc: 0.59375 Train acc: 0.72265625\n","Epoch: 230 Cost: 0.552969456 Dev acc: 0.62109375 Train acc: 0.72265625\n","Epoch: 235 Cost: 0.544264555 Dev acc: 0.58984375 Train acc: 0.71875\n","Epoch: 240 Cost: 0.543156445 Dev acc: 0.609375 Train acc: 0.7421875\n","Epoch: 245 Cost: 0.535833895 Dev acc: 0.6015625 Train acc: 0.76171875\n","Epoch: 250 Cost: 0.533747077 Dev acc: 0.5625 Train acc: 0.76171875\n","Epoch: 255 Cost: 0.537607253 Dev acc: 0.59375 Train acc: 0.74609375\n","Epoch: 260 Cost: 0.530593395 Dev acc: 0.58984375 Train acc: 0.74609375\n","Epoch: 265 Cost: 0.526977181 Dev acc: 0.59375 Train acc: 0.703125\n","Epoch: 270 Cost: 0.513751745 Dev acc: 0.56640625 Train acc: 0.77734375\n","Epoch: 275 Cost: 0.523941 Dev acc: 0.609375 Train acc: 0.76953125\n","Epoch: 280 Cost: 0.516184747 Dev acc: 0.59375 Train acc: 0.7734375\n","Epoch: 285 Cost: 0.511911809 Dev acc: 0.59765625 Train acc: 0.75390625\n","Epoch: 290 Cost: 0.495846301 Dev acc: 0.62890625 Train acc: 0.734375\n","Epoch: 295 Cost: 0.49457258 Dev acc: 0.546875 Train acc: 0.74609375\n","Epoch: 300 Cost: 0.491412044 Dev acc: 0.5546875 Train acc: 0.71875\n","Epoch: 305 Cost: 0.495054692 Dev acc: 0.609375 Train acc: 0.7421875\n","Epoch: 310 Cost: 0.48476547 Dev acc: 0.54296875 Train acc: 0.7109375\n","Epoch: 315 Cost: 0.481196 Dev acc: 0.578125 Train acc: 0.79296875\n","Epoch: 320 Cost: 0.482584924 Dev acc: 0.59375 Train acc: 0.8046875\n","Epoch: 325 Cost: 0.486271024 Dev acc: 0.60546875 Train acc: 0.734375\n","Epoch: 330 Cost: 0.467784017 Dev acc: 0.5390625 Train acc: 0.80078125\n","Epoch: 335 Cost: 0.460992694 Dev acc: 0.6015625 Train acc: 0.8203125\n","Epoch: 340 Cost: 0.471350253 Dev acc: 0.578125 Train acc: 0.76953125\n","Epoch: 345 Cost: 0.45762521 Dev acc: 0.609375 Train acc: 0.7578125\n","Epoch: 350 Cost: 0.461484 Dev acc: 0.59375 Train acc: 0.7734375\n","Epoch: 355 Cost: 0.441222161 Dev acc: 0.59375 Train acc: 0.82421875\n","Epoch: 360 Cost: 0.473396569 Dev acc: 0.5546875 Train acc: 0.80078125\n","Epoch: 365 Cost: 0.432947963 Dev acc: 0.63671875 Train acc: 0.76953125\n","Epoch: 370 Cost: 0.437023848 Dev acc: 0.625 Train acc: 0.81640625\n","Epoch: 375 Cost: 0.428068817 Dev acc: 0.6328125 Train acc: 0.8515625\n","Epoch: 380 Cost: 0.432834923 Dev acc: 0.62109375 Train acc: 0.76171875\n","Epoch: 385 Cost: 0.421137154 Dev acc: 0.6328125 Train acc: 0.8359375\n","Epoch: 390 Cost: 0.41937992 Dev acc: 0.60546875 Train acc: 0.84375\n","Epoch: 395 Cost: 0.419204623 Dev acc: 0.59375 Train acc: 0.8515625\n","Epoch: 400 Cost: 0.415825665 Dev acc: 0.62890625 Train acc: 0.85546875\n","Epoch: 405 Cost: 0.400518537 Dev acc: 0.60546875 Train acc: 0.85546875\n","Epoch: 410 Cost: 0.408633798 Dev acc: 0.53515625 Train acc: 0.8203125\n","Epoch: 415 Cost: 0.398474276 Dev acc: 0.59765625 Train acc: 0.8203125\n","Epoch: 420 Cost: 0.38800016 Dev acc: 0.64453125 Train acc: 0.80859375\n","Epoch: 425 Cost: 0.376536787 Dev acc: 0.5859375 Train acc: 0.86328125\n","Epoch: 430 Cost: 0.380303919 Dev acc: 0.62890625 Train acc: 0.8359375\n","Epoch: 435 Cost: 0.384452492 Dev acc: 0.609375 Train acc: 0.875\n","Epoch: 440 Cost: 0.404471815 Dev acc: 0.625 Train acc: 0.8359375\n","Epoch: 445 Cost: 0.380444378 Dev acc: 0.578125 Train acc: 0.86328125\n","Epoch: 450 Cost: 0.368268818 Dev acc: 0.6015625 Train acc: 0.86328125\n","Epoch: 455 Cost: 0.437023401 Dev acc: 0.58203125 Train acc: 0.8515625\n","Epoch: 460 Cost: 0.383697301 Dev acc: 0.58203125 Train acc: 0.78515625\n","Epoch: 465 Cost: 0.351550728 Dev acc: 0.6015625 Train acc: 0.921875\n","Epoch: 470 Cost: 0.352601379 Dev acc: 0.609375 Train acc: 0.8046875\n","Epoch: 475 Cost: 0.328129 Dev acc: 0.6171875 Train acc: 0.89453125\n","Epoch: 480 Cost: 0.369517833 Dev acc: 0.6171875 Train acc: 0.921875\n","Epoch: 485 Cost: 0.338167459 Dev acc: 0.6171875 Train acc: 0.89453125\n","Epoch: 490 Cost: 0.352269918 Dev acc: 0.62109375 Train acc: 0.87109375\n","Epoch: 495 Cost: 0.415036291 Dev acc: 0.5859375 Train acc: 0.85546875\n","Epoch: 500 Cost: 0.328033239 Dev acc: 0.640625 Train acc: 0.88671875\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kjBUT-D2spyT","colab_type":"text"},"source":["# Atribution:\n","Adapted by Oier Lopez de Lacalle and Olatz Perez de Viñaspre, based on a notebook by Sam Bowman at NYU"]},{"cell_type":"code","metadata":{"id":"Z0Gu_Dy8jpGr","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}